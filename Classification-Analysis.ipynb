{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-ivory",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#importing my libraries\n",
    "import matplotlib.pyplot as plt                      \n",
    "import pandas as pd                                  \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import roc_auc_score       \n",
    "from sklearn.metrics import confusion_matrix  \n",
    "import statsmodels.formula.api as smf \n",
    "import numpy as np\n",
    "\n",
    "# CART model packages\n",
    "from sklearn.tree import DecisionTreeClassifier   \n",
    "from sklearn.tree import export_graphviz   \n",
    "from io import StringIO\n",
    "from IPython.display import Image                  \n",
    "import pydotplus \n",
    "\n",
    "# new packages\n",
    "from sklearn.model_selection import RandomizedSearchCV    \n",
    "from sklearn.metrics import make_scorer  \n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier     # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gbm\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Datasets/Apprentice_Chef_Dataset.xlsx'\n",
    "\n",
    "# reading the file into Python\n",
    "data_df = pd.read_excel(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-twist",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-pointer",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "data_df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-classics",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# creating a dropped dataset to visualize 'FAMILY_NAME'\n",
    "df_dropped = data_df.dropna()\n",
    "\n",
    "#check if there is any missing values in the dropped dataset\n",
    "df_dropped.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-kitty",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# creating an imputation value\n",
    "fill = 'Unknown'\n",
    "\n",
    "# imputing 'FAMILY_NAME'\n",
    "data_df['FAMILY_NAME'] = data_df['FAMILY_NAME'].fillna(fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-spray",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Creating New Calculated Variables\n",
    "data_df['MOBILE_LOGINS_RATE'] = data_df['MOBILE_LOGINS'] / data_df['TOTAL_MEALS_ORDERED']\n",
    "data_df['CANCELLATION_RATE'] = (data_df['CANCELLATIONS_BEFORE_NOON'] + data_df['CANCELLATIONS_AFTER_NOON'])/ data_df['TOTAL_MEALS_ORDERED']\n",
    "data_df['CATVIEWS_CLICKS_RATIO'] = data_df['PRODUCT_CATEGORIES_VIEWED'] / data_df['AVG_CLICKS_PER_VISIT']\n",
    "data_df['CATVIEWS_VISIT_RATIO'] = data_df['PRODUCT_CATEGORIES_VIEWED'] / data_df['AVG_TIME_PER_SITE_VISIT']\n",
    "\n",
    "\n",
    "# checking result\n",
    "data_df.loc[ : , ['MOBILE_LOGINS_RATE','CANCELLATION_RATE',\n",
    "                 'CATVIEWS_CLICKS_RATIO',\n",
    "                 'CATVIEWS_VISIT_RATIO']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-operation",
   "metadata": {},
   "source": [
    "### Continuous Variables: Trend Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-huntington",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Log transformation on the continuos variables\n",
    "data_df['log_REVENUE'] = np.log10(data_df['REVENUE'])\n",
    "data_df['log_AVG_TIME_PER_SITE_VISIT'] = np.log10(data_df['AVG_TIME_PER_SITE_VISIT'])\n",
    "data_df['log_AVG_PREP_VID_TIME'] = np.log10(data_df['AVG_PREP_VID_TIME'])\n",
    "data_df['log_TOTAL_MEALS_ORDERED'] = np.log10(data_df['TOTAL_MEALS_ORDERED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-state",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# dummy variable for the 2 features\n",
    "data_df['has_TOTAL_PHOTOS_VIEWED'] = 0\n",
    "data_df['has_WEEKLY_PLAN'] = 0\n",
    "\n",
    "\n",
    "for index, value in data_df.iterrows():\n",
    "    \n",
    "    # TOTAL_PHOTOS_VIEWED\n",
    "    if data_df.loc[index, 'TOTAL_PHOTOS_VIEWED'] > 0:\n",
    "        data_df.loc[index, 'has_TOTAL_PHOTOS_VIEWED'] = 1\n",
    "\n",
    "\n",
    "    # Second_Flr_SF\n",
    "    if data_df.loc[index, 'WEEKLY_PLAN'] > 0:\n",
    "        data_df.loc[index, 'has_WEEKLY_PLAN'] = 1\n",
    "        \n",
    "        \n",
    "# checking results\n",
    "data_df['has_TOTAL_PHOTOS_VIEWED'].value_counts(normalize = False).sort_index()\n",
    "data_df['has_WEEKLY_PLAN'].value_counts(normalize = False).sort_index()\n",
    "\n",
    "data_df[['has_TOTAL_PHOTOS_VIEWED', 'has_WEEKLY_PLAN']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-speaking",
   "metadata": {},
   "source": [
    "### Interval / Count Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a log transformation on interval and count variables that are skewed or has 0\n",
    "data_df['log_UNIQUE_MEALS_PURCH'] = np.log10(data_df['UNIQUE_MEALS_PURCH'])+ 0.01\n",
    "data_df['log_CONTACTS_W_CUSTOMER_SERVICE'] = np.log10(data_df['CONTACTS_W_CUSTOMER_SERVICE']) + 0.01\n",
    "data_df['log_PRODUCT_CATEGORIES_VIEWED'] = np.log10(data_df['PRODUCT_CATEGORIES_VIEWED'])+ 0.01\n",
    "data_df['log_LARGEST_ORDER_SIZE'] = np.log10(data_df['LARGEST_ORDER_SIZE']) + 0.01\n",
    "data_df['log_AVG_CLICKS_PER_VISIT'] = np.log10(data_df['AVG_CLICKS_PER_VISIT']) + 0.01\n",
    "data_df['log_MEDIAN_MEAL_RATING'] = np.log10(data_df['MEDIAN_MEAL_RATING'])+ 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-neutral",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Categorizing the Order size into 3 (Small, Medium, Large)\n",
    "\n",
    "data_df['ORDER_SIZE_SMALL'] = 0\n",
    "data_df['ORDER_SIZE_MEDIUM'] = 0\n",
    "data_df['ORDER_SIZE_LARGE'] = 0\n",
    "\n",
    "for index, value in data_df.iterrows():\n",
    "    if data_df.loc[index, 'LARGEST_ORDER_SIZE'] >= 8.00:\n",
    "        data_df.loc[index, 'ORDER_SIZE_LARGE'] = 1\n",
    "    elif data_df.loc[index, 'LARGEST_ORDER_SIZE'] >= 7.00:\n",
    "        data_df.loc[index, 'ORDER_SIZE_MEDIUM'] = 1\n",
    "    elif data_df.loc[index, 'LARGEST_ORDER_SIZE'] >= 4.00:\n",
    "        data_df.loc[index, 'ORDER_SIZE_SMALL'] = 1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-savannah",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Creating new columns for dummy variable\n",
    "data_df['has_CANCELLATIONS_BEFORE_NOON']  = 0\n",
    "data_df['has_CANCELLATIONS_AFTER_NOON']   = 0\n",
    "data_df['has_MASTER_CLASSES_ATTENDED']    = 0\n",
    "data_df['has_EARLY_DELIVERIES']           = 0\n",
    "data_df['has_LATE_DELIVERIES']             = 0\n",
    "data_df['no_LATE_DELIVERIES']              = 0\n",
    "\n",
    "        \n",
    "# for loop to declare 0 and 1 \n",
    "for index, value in data_df.iterrows():\n",
    "    \n",
    "    # cancellations_before_noon\n",
    "    if data_df.loc[index,'CANCELLATIONS_BEFORE_NOON'] > 0:\n",
    "        data_df.loc[index, 'has_CANCELLATIONS_BEFORE_NOON'] = 1\n",
    "\n",
    "    # cancellations_after_noon\n",
    "    if data_df.loc[index, 'CANCELLATIONS_AFTER_NOON'] > 0:\n",
    "        data_df.loc[index, 'has_CANCELLATIONS_AFTER_NOON'] = 1\n",
    "        \n",
    "    # master_classes_attended\n",
    "    if data_df.loc[index, 'MASTER_CLASSES_ATTENDED'] > 0:\n",
    "        data_df.loc[index, 'has_MASTER_CLASSES_ATTENDED'] = 1\n",
    "        \n",
    "    # early_deliveries\n",
    "    if data_df.loc[index, 'EARLY_DELIVERIES'] > 0:\n",
    "        data_df.loc[index, 'has_EARLY_DELIVERIES'] = 1\n",
    "    \n",
    "    # late_deliveries\n",
    "    if data_df.loc[index, 'LATE_DELIVERIES'] > 0:\n",
    "        data_df.loc[index, 'has_LATE_DELIVERIES'] = 1\n",
    "\n",
    "    # no_late_deliveries\n",
    "    if data_df.loc[index,'LATE_DELIVERIES'] == 0:\n",
    "        data_df.loc[index, 'no_LATE_DELIVERIES'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-wagon",
   "metadata": {},
   "source": [
    "## Categorial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-bride",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Splitting emails\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "# looping over each email address\n",
    "for index, col in data_df.iterrows():\n",
    "    \n",
    "    # splitting email domain at '@'\n",
    "    split_email = data_df.loc[index, 'EMAIL'].split(sep = '@')\n",
    "    \n",
    "    # appending placeholder_lst with the results\n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "\n",
    "# converting placeholder_lst into a DataFrame \n",
    "emails = pd.DataFrame(placeholder_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-genius",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# renaming column to concatenate\n",
    "emails.columns = ['0' , 'EMAIL_domain']\n",
    "\n",
    "\n",
    "# concatenating personal_email_domain with friends DataFrame\n",
    "data_df = pd.concat([data_df, emails['EMAIL_domain']],\n",
    "                     axis = 1)\n",
    "\n",
    "\n",
    "# printing value counts of personal_email_domain\n",
    "data_df.loc[: ,'EMAIL_domain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-youth",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Creating domain types \n",
    "\n",
    "# email domain types\n",
    "professional_emails = ['@amex.com','@jnj.com', '@merck.com', '@cocacola.com','@mcdonalds.com', '@apple.com',\n",
    "                              '@nike.com','@ge.org','@dupont.com','@ibm.com','@chevron.com','@microsoft.com','@exxon.com','@unitedhealth.com',\n",
    "                              '@travelers.com','@boeing.com','@mmm.com','@caterpillar.com','@verizon.com','@pg.com',\n",
    "                              '@walmart.com','@disney.com','@pfizer.com','@visa.com','@jpmorgan.com','@cisco.com',\n",
    "                              '@unitedtech.com','@goldmansacs.com','@homedepot.com','@intel.com']\n",
    "\n",
    "personal_emails  = ['@gmail.com', '@yahoo.com', '@protonmail.com']\n",
    "\n",
    "\n",
    "junk_email_domains       = ['@me.com',\n",
    "                            '@aol.com',\n",
    "                            '@hotmail.com',\n",
    "                            '@live.com',\n",
    "                            '@msn.com',\n",
    "                            '@passport.com']\n",
    "\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "\n",
    "# looping to group observations by domain type\n",
    "for domain in data_df['EMAIL_domain']:\n",
    "    \n",
    "    if '@' + domain in professional_emails:\n",
    "        placeholder_lst.append('professional')\n",
    "        \n",
    "\n",
    "    elif '@' + domain in personal_emails:\n",
    "        placeholder_lst.append('personal')\n",
    "        \n",
    "    elif '@' + domain in junk_email_domains:\n",
    "        placeholder_lst.append('junk')\n",
    "    \n",
    "    else:\n",
    "            print('Unknown')\n",
    "\n",
    "\n",
    "# concatenating with original DataFrame\n",
    "data_df['domain_group'] = pd.Series(placeholder_lst)\n",
    "\n",
    "\n",
    "# checking results\n",
    "data_df['domain_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-photographer",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# one hot encoding for email domain categorial variable\n",
    "one_hot_domain       = pd.get_dummies(data_df['domain_group'])\n",
    "\n",
    "\n",
    "# dropping categorical variables after they've been encoded\n",
    "data_df = data_df.drop('EMAIL', axis = 1)\n",
    "data_df = data_df.drop('domain_group', axis = 1)\n",
    "data_df = data_df.drop('EMAIL_domain', axis = 1)\n",
    "\n",
    "# joining codings together\n",
    "data_df = data_df.join([one_hot_domain])\n",
    "\n",
    "\n",
    "# saving new columns\n",
    "new_columns = data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-runner",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Counting the number of names\n",
    "\n",
    "def mv_flagger(df):\n",
    "    \"\"\"\n",
    "Flags all columns that have missing values with 'm-COLUMN_NAME'.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "df : DataFrame to flag missing values\n",
    "\n",
    "\n",
    "RETURNS\n",
    "-------\n",
    "DataFrame with missing value flags.\"\"\"\n",
    "\n",
    "\n",
    "    for col in df:\n",
    "\n",
    "        if df[col].isnull().astype(int).sum() > 0:\n",
    "            df['m_'+col] = df[col].isnull().astype(int)\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "# text_split_feature\n",
    "#########################\n",
    "def text_split_feature(col, df, sep=' ', new_col_name='NUM_OF_NAMES'):\n",
    "    \"\"\"\n",
    "Splits values in a string Series (as part of a DataFrame) and sums the number\n",
    "of resulting items. Automatically appends summed column to original DataFrame.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "col          : column to split\n",
    "df           : DataFrame where column is located\n",
    "sep          : string sequence to split by, default ' '\n",
    "new_col_name : name of new column after summing split, default\n",
    "               'number_of_names'\n",
    "\"\"\"\n",
    "    \n",
    "    df[new_col_name] = 0\n",
    "    \n",
    "    \n",
    "    for index, val in df.iterrows():\n",
    "        df.loc[index, new_col_name] = len(df.loc[index, col].split(sep = ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-nigeria",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# calling text_split_feature\n",
    "text_split_feature(col = 'NAME',\n",
    "                   df  = data_df)\n",
    "\n",
    "\n",
    "# checking results\n",
    "data_df['NUM_OF_NAMES'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-intelligence",
   "metadata": {},
   "source": [
    "## ML: Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-beach",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# optimal_neighbors\n",
    "\n",
    "def optimal_neighbors(x_data,\n",
    "                      y_data,\n",
    "                      standardize = True,\n",
    "                      pct_test=0.25,\n",
    "                      seed=219,\n",
    "                      response_type='reg',\n",
    "                      max_neighbors=20,\n",
    "                      show_viz=True):\n",
    "    \"\"\"\n",
    "Exhaustively compute training and testing results for KNN across\n",
    "[1, max_neighbors]. Outputs the maximum test score and (by default) a\n",
    "visualization of the results.\n",
    "PARAMETERS\n",
    "----------\n",
    "x_data        : explanatory variable data\n",
    "y_data        : response variable\n",
    "standardize   : whether or not to standardize the X data, default True\n",
    "pct_test      : test size for training and validation from (0,1), default 0.25\n",
    "seed          : random seed to be used in algorithm, default 219\n",
    "response_type : type of neighbors algorithm to use, default 'reg'\n",
    "    Use 'reg' for regression (KNeighborsRegressor)\n",
    "    Use 'class' for classification (KNeighborsClassifier)\n",
    "max_neighbors : maximum number of neighbors in exhaustive search, default 20\n",
    "show_viz      : display or surpress k-neigbors visualization, default True\n",
    "\"\"\"    \n",
    "    \n",
    "    \n",
    "    if standardize == True:\n",
    "        # optionally standardizing x_data\n",
    "        scaler             = StandardScaler()\n",
    "        scaler.fit(x_data)\n",
    "        x_scaled           = scaler.transform(x_data)\n",
    "        x_scaled_df        = pd.DataFrame(x_scaled)\n",
    "        x_data             = x_scaled_df\n",
    "\n",
    "\n",
    "\n",
    "    # train-test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data,\n",
    "                                                        y_data,\n",
    "                                                        test_size = pct_test,\n",
    "                                                        random_state = seed)\n",
    "\n",
    "\n",
    "    # creating lists for training set accuracy and test set accuracy\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    \n",
    "    # setting neighbor range\n",
    "    neighbors_settings = range(1, max_neighbors + 1)\n",
    "\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # building the model based on response variable type\n",
    "        if response_type == 'reg':\n",
    "            clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "            clf.fit(x_train, y_train)\n",
    "            \n",
    "        elif response_type == 'class':\n",
    "            clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "            clf.fit(x_train, y_train)            \n",
    "            \n",
    "        else:\n",
    "            print(\"Error: response_type must be 'reg' or 'class'\")\n",
    "        \n",
    "        \n",
    "        # recording the training set accuracy\n",
    "        training_accuracy.append(clf.score(x_train, y_train))\n",
    "    \n",
    "        # recording the generalization accuracy\n",
    "        test_accuracy.append(clf.score(x_test, y_test))\n",
    "\n",
    "\n",
    "    # optionally displaying visualization\n",
    "    if show_viz == True:\n",
    "        # plotting the visualization\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "        plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "        plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(\"n_neighbors\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # returning optimal number of neighbors\n",
    "    print(f\"The optimal number of neighbors is: {test_accuracy.index(max(test_accuracy))+1}\")\n",
    "    return test_accuracy.index(max(test_accuracy))+1\n",
    "\n",
    "\n",
    "\n",
    "# visual_cm\n",
    "\n",
    "def visual_cm(true_y, pred_y, labels = None):\n",
    "    \"\"\"\n",
    "Creates a visualization of a confusion matrix.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "true_y : true values for the response variable\n",
    "pred_y : predicted values for the response variable\n",
    "labels : , default None\n",
    "    \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-collection",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Checking correlations with Y variable\n",
    "display(data_df.loc[:, \"CROSS_SELL_SUCCESS\"].value_counts())\n",
    "\n",
    "corr_scores = data_df.corr()\n",
    "\n",
    "corr_scores.loc[:, \"CROSS_SELL_SUCCESS\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-tournament",
   "metadata": {},
   "source": [
    "# TRAIN - TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-bridal",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# declaring explanatory variables\n",
    "data_df_drop = ['CROSS_SELL_SUCCESS','NAME','FIRST_NAME', 'FAMILY_NAME']\n",
    "\n",
    "data_df_x = data_df.drop(data_df_drop, axis = 1)\n",
    "\n",
    "# declaring response variable\n",
    "data_df_y = data_df.loc[ : , 'CROSS_SELL_SUCCESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-textbook",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# train-test split with stratification\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            data_df_x,\n",
    "            data_df_y,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = data_df_y)\n",
    "\n",
    "\n",
    "# merging training data for statsmodels\n",
    "data_df_train = pd.concat([x_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-thesis",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "for val in data_df_x:\n",
    "    print(f\" {val} + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-hostel",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logit_full = smf.logit(formula = \"\"\" CROSS_SELL_SUCCESS ~  REVENUE + \n",
    " TOTAL_MEALS_ORDERED + \n",
    " UNIQUE_MEALS_PURCH + \n",
    " CONTACTS_W_CUSTOMER_SERVICE + \n",
    " PRODUCT_CATEGORIES_VIEWED + \n",
    " AVG_TIME_PER_SITE_VISIT + \n",
    " MOBILE_NUMBER + \n",
    " CANCELLATIONS_BEFORE_NOON + \n",
    " CANCELLATIONS_AFTER_NOON + \n",
    " TASTES_AND_PREFERENCES + \n",
    " PC_LOGINS + \n",
    " MOBILE_LOGINS + \n",
    " WEEKLY_PLAN + \n",
    " EARLY_DELIVERIES + \n",
    " LATE_DELIVERIES + \n",
    " PACKAGE_LOCKER + \n",
    " REFRIGERATED_LOCKER + \n",
    " AVG_PREP_VID_TIME + \n",
    " LARGEST_ORDER_SIZE + \n",
    " MASTER_CLASSES_ATTENDED + \n",
    " MEDIAN_MEAL_RATING + \n",
    " AVG_CLICKS_PER_VISIT + \n",
    " TOTAL_PHOTOS_VIEWED + \n",
    " MOBILE_LOGINS_RATE + \n",
    " CANCELLATION_RATE + \n",
    " CATVIEWS_CLICKS_RATIO + \n",
    " CATVIEWS_VISIT_RATIO + \n",
    " log_REVENUE + \n",
    " log_AVG_TIME_PER_SITE_VISIT + \n",
    " log_AVG_PREP_VID_TIME + \n",
    " log_TOTAL_MEALS_ORDERED + \n",
    " has_TOTAL_PHOTOS_VIEWED + \n",
    " has_WEEKLY_PLAN + \n",
    " ORDER_SIZE_SMALL + \n",
    " ORDER_SIZE_MEDIUM + \n",
    " ORDER_SIZE_LARGE + \n",
    " has_CANCELLATIONS_BEFORE_NOON + \n",
    " has_CANCELLATIONS_AFTER_NOON + \n",
    " has_MASTER_CLASSES_ATTENDED + \n",
    " has_EARLY_DELIVERIES + \n",
    " has_LATE_DELIVERIES + \n",
    " no_LATE_DELIVERIES + \n",
    " junk + \n",
    " personal + \n",
    " professional + \n",
    " NUM_OF_NAMES\n",
    "                                     \"\"\",\n",
    " data    = data_df_train)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "logit_full = logit_full.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "logit_full.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-tumor",
   "metadata": {},
   "source": [
    "# CANDIDATE DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-light",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# creating a dictionary to store candidate models\n",
    "# I tried different variables for different models and picked which set will perform better\n",
    "\n",
    "candidate_dict = {\n",
    "\n",
    " # full model\n",
    " 'logit_full'   : ['REVENUE',\n",
    "'TOTAL_MEALS_ORDERED',\n",
    "'UNIQUE_MEALS_PURCH',\n",
    "'CONTACTS_W_CUSTOMER_SERVICE',\n",
    "'PRODUCT_CATEGORIES_VIEWED',\n",
    "'AVG_TIME_PER_SITE_VISIT',\n",
    "'MOBILE_NUMBER',\n",
    "'CANCELLATIONS_BEFORE_NOON',\n",
    "'CANCELLATIONS_AFTER_NOON',\n",
    "'TASTES_AND_PREFERENCES',\n",
    "'PC_LOGINS',\n",
    "'MOBILE_LOGINS',\n",
    "'WEEKLY_PLAN',\n",
    "'EARLY_DELIVERIES',\n",
    "'LATE_DELIVERIES',\n",
    "'PACKAGE_LOCKER',\n",
    "'REFRIGERATED_LOCKER',\n",
    "'AVG_PREP_VID_TIME',\n",
    "'LARGEST_ORDER_SIZE',\n",
    "'MASTER_CLASSES_ATTENDED',\n",
    "'MEDIAN_MEAL_RATING',\n",
    "'AVG_CLICKS_PER_VISIT',\n",
    "'TOTAL_PHOTOS_VIEWED',\n",
    "'MOBILE_LOGINS_RATE',\n",
    "'CANCELLATION_RATE',\n",
    "'has_CANCELLATIONS_BEFORE_NOON',\n",
    "'has_CANCELLATIONS_AFTER_NOON',\n",
    "'has_WEEKLY_PLAN',\n",
    "'has_MASTER_CLASSES_ATTENDED',\n",
    "'has_EARLY_DELIVERIES',\n",
    "'has_LATE_DELIVERIES',\n",
    "'no_LATE_DELIVERIES',\n",
    "'professional',\n",
    "'personal',\n",
    "'junk',\n",
    "'MOBILE_TO_PC_RATIO',\n",
    "'CATVIEWS_CLICKS_RATIO',\n",
    "'CATVIEWS_VISIT_RATIO'],\n",
    " \n",
    "\n",
    "            \n",
    "# significant variables only (set 1)\n",
    " 'logit_sig'    : ['CONTACTS_W_CUSTOMER_SERVICE' ,\n",
    "                   'MOBILE_NUMBER' ,\n",
    "                   'TASTES_AND_PREFERENCES' ,\n",
    "                   'CANCELLATIONS_BEFORE_NOON' ,\n",
    "                   'PC_LOGINS' , \n",
    "                   'EARLY_DELIVERIES' ,\n",
    "                   'REFRIGERATED_LOCKER' ,\n",
    "                   'NUM_OF_NAMES',\n",
    "                   'junk'],\n",
    "\n",
    "\n",
    "# significant variables only (set 1)\n",
    " 'logit_sig2'    : ['TOTAL_MEALS_ORDERED', 'MOBILE_LOGINS',\n",
    "                    'WEEKLY_PLAN', 'has_MASTER_CLASSES_ATTENDED','PRODUCT_CATEGORIES_VIEWED', \n",
    "                    'CONTACTS_W_CUSTOMER_SERVICE',\n",
    "                   'MOBILE_NUMBER', 'TASTES_AND_PREFERENCES', 'CANCELLATIONS_BEFORE_NOON',\n",
    "                   'PC_LOGINS',  'EARLY_DELIVERIES','REFRIGERATED_LOCKER',\n",
    "                   'NUM_OF_NAMES','junk'],\n",
    "        \n",
    "    \n",
    "           \n",
    "# significant variables only (set 1)\n",
    " 'logit_sig3'    : ['TOTAL_MEALS_ORDERED', 'MOBILE_LOGINS','log_REVENUE',\n",
    "                    'WEEKLY_PLAN', 'has_MASTER_CLASSES_ATTENDED','PRODUCT_CATEGORIES_VIEWED', \n",
    "                    'CONTACTS_W_CUSTOMER_SERVICE', 'MOBILE_NUMBER',\n",
    "                   'TASTES_AND_PREFERENCES','CANCELLATIONS_BEFORE_NOON',\n",
    "                   'PC_LOGINS', 'EARLY_DELIVERIES','REFRIGERATED_LOCKER','has_EARLY_DELIVERIES',\n",
    "                   'NUM_OF_NAMES', 'junk','log_AVG_PREP_VID_TIME', 'ORDER_SIZE_SMALL','ORDER_SIZE_MEDIUM']\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-score",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-naples",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "data_df_x   =  data_df.loc[ : , candidate_dict['logit_sig']]\n",
    "data_df_y =  data_df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data_df_x,\n",
    "            data_df_y,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = data_df_y)\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            C = 1,\n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('LogReg Training ACCURACY:', logreg_fit.score(X_train, y_train).round(4))\n",
    "print('LogReg Testing  ACCURACY:', logreg_fit.score(X_test, y_test).round(4))\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(X_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('LogReg Train-Test Gap   :', abs(logreg_train_score - logreg_test_score).round(4))\n",
    "logreg_test_gap = abs(logreg_train_score - logreg_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-campaign",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {logreg_tn}\n",
    "False Positives: {logreg_fp}\n",
    "False Negatives: {logreg_fn}\n",
    "True Positives : {logreg_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-given",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# area under the roc curve (auc)\n",
    "print(roc_auc_score(y_true  = y_test,\n",
    "                    y_score = logreg_pred).round(decimals = 4))\n",
    "\n",
    "\n",
    "# saving AUC score for future use\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = logreg_pred).round(decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-yemen",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "logreg_model_values = zip(data_df[candidate_dict['logit_sig']].columns,\n",
    "                          logreg_fit.coef_.ravel().round(decimals = 2))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "logreg_model_lst = [('intercept', logreg_fit.intercept_[0].round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in logreg_model_values:\n",
    "    logreg_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in logreg_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-syndrome",
   "metadata": {},
   "source": [
    "# Classification: Full Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-pencil",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "full_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(x_train,\n",
    "                                                    y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(x_test,\n",
    "                                                    y_test).round(4))\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-parade",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "full_tree_tn, \\\n",
    "full_tree_fp, \\\n",
    "full_tree_fn, \\\n",
    "full_tree_tp = confusion_matrix(y_true = y_test, y_pred = full_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {full_tree_tn}\n",
    "False Positives: {full_tree_fp}\n",
    "False Negatives: {full_tree_fn}\n",
    "True Positives : {full_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-commerce",
   "metadata": {},
   "source": [
    "# Pruned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-blanket",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "pruned_tree = DecisionTreeClassifier(max_depth = 4,\n",
    "                                     min_samples_leaf = 25,\n",
    "                                     random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "pruned_tree_fit  = pruned_tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "pruned_tree_pred = pruned_tree_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', pruned_tree_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', pruned_tree_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = pruned_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "pruned_tree_train_score = pruned_tree_fit.score(X_train, y_train).round(4) # accuracy\n",
    "pruned_tree_test_score  = pruned_tree_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving auc score\n",
    "pruned_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                        y_score = pruned_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-invalid",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "pruned_tree_tn, \\\n",
    "pruned_tree_fp, \\\n",
    "pruned_tree_fn, \\\n",
    "pruned_tree_tp = confusion_matrix(y_true = y_test, y_pred = pruned_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {pruned_tree_tn}\n",
    "False Positives: {pruned_tree_fp}\n",
    "False Negatives: {pruned_tree_fn}\n",
    "True Positives : {pruned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-armstrong",
   "metadata": {},
   "source": [
    "# RESULTS (WITHOUT TUNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-bidding",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# This is NOT my final results\n",
    "\n",
    "# comparing results\n",
    "print(f\"\"\"\n",
    "Model         AUC Score      TN, FP, FN, TP\n",
    "-----         ---------      --------------\n",
    "Logistic      {logreg_auc_score}         {logreg_tn, logreg_fp, logreg_fn, logreg_tp}\n",
    "Full Tree     {full_tree_auc_score}         {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}\n",
    "Pruned Tree   {pruned_tree_auc_score}         {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Name'    : ['Logistic', 'Full Tree', 'Pruned Tree'],\n",
    "           \n",
    "    'AUC Score' : [logreg_auc_score, full_tree_auc_score, pruned_tree_auc_score],\n",
    "    \n",
    "    'Training Accuracy' : [logreg_train_score, full_tree_train_score,\n",
    "                           pruned_tree_train_score],\n",
    "           \n",
    "    'Testing Accuracy'  : [logreg_test_score, full_tree_test_score,\n",
    "                           pruned_tree_test_score],\n",
    "\n",
    "    'Confusion Matrix'  : [(logreg_tn, logreg_fp, logreg_fn, logreg_tp),\n",
    "                           (full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp),\n",
    "                           (pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-cedar",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-newspaper",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# RandomizedSearchCV\n",
    "# declaring a hyperparameter space\n",
    "C_space          = pd.np.arange(0.1, 5.0, 0.1)\n",
    "warm_start_space = [True, False]\n",
    "solver_space     = ['newton-cg', 'sag', 'lbfgs']\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'C'          : C_space,\n",
    "              'warm_start' : warm_start_space,\n",
    "              'solver'     : solver_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "lr_tuned = LogisticRegression(random_state = 219,\n",
    "                              max_iter     = 1000)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "lr_tuned_cv = RandomizedSearchCV(estimator           = lr_tuned,   # the model object\n",
    "                                 param_distributions = param_grid, # parameters to tune\n",
    "                                 cv                  = 3,          # how many folds in cross-validation\n",
    "                                 n_iter              = 250,        # number of combinations of hyperparameters to try\n",
    "                                 random_state        = 219,        # starting point for random sequence\n",
    "                                 scoring = make_scorer(\n",
    "                                           roc_auc_score,\n",
    "                                           needs_threshold = False)) # scoring criteria (AUC)\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "lr_tuned_cv.fit(data_df_x, data_df_y)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", lr_tuned_cv.best_params_)\n",
    "print(\"Tuned CV AUC      :\", lr_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-pledge",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "lr_tuned = lr_tuned_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step is not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "lr_tuned_pred = lr_tuned.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('LR Tuned Training ACCURACY:', lr_tuned.score(X_train, y_train).round(4))\n",
    "print('LR Tuned Testing  ACCURACY:', lr_tuned.score(X_test, y_test).round(4))\n",
    "print('LR Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = lr_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "lr_tuned_train_score = lr_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "lr_tuned_test_score  = lr_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "lr_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = lr_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-julian",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning on Classification Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-wagner",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "criterion_space = ['gini', 'entropy']\n",
    "splitter_space  = ['best', 'random']\n",
    "depth_space     = pd.np.arange(1, 25, 1)\n",
    "leaf_space      = pd.np.arange(1, 100, 1)\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'criterion'        : criterion_space,\n",
    "              'splitter'         : splitter_space,\n",
    "              'max_depth'        : depth_space,\n",
    "              'min_samples_leaf' : leaf_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "tuned_tree = DecisionTreeClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# RandomizedSearchCV object\n",
    "tuned_tree_cv = RandomizedSearchCV(estimator             = tuned_tree,\n",
    "                                   param_distributions   = param_grid,\n",
    "                                   cv                    = 3,\n",
    "                                   n_iter                = 1000,\n",
    "                                   random_state          = 219,\n",
    "                                   scoring = make_scorer(roc_auc_score,\n",
    "                                             needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "tuned_tree_cv.fit(data_df_x, data_df_y)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", tuned_tree_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", tuned_tree_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-soundtrack",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "tree_tuned = tuned_tree_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step is not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "tree_tuned_pred = tree_tuned.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', tree_tuned.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', tree_tuned.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = tree_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "tree_tuned_train_score = tree_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "tree_tuned_test_score  = tree_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Train-Test Gap   :', abs(tree_tuned_train_score - \\\n",
    "                                       tree_tuned_test_score).round(4))\n",
    "\n",
    "tree_tuned_test_gap = abs(tree_tuned_train_score - tree_tuned_test_score).round(4)\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "tree_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = tree_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-noise",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "tuned_tree_tn, \\\n",
    "tuned_tree_fp, \\\n",
    "tuned_tree_fn, \\\n",
    "tuned_tree_tp = confusion_matrix(y_true = y_test, y_pred = tree_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {tuned_tree_tn}\n",
    "False Positives: {tuned_tree_fp}\n",
    "False Negatives: {tuned_tree_fn}\n",
    "True Positives : {tuned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-compiler",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-colleague",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "data_df_x   =  data_df.loc[ : , candidate_dict['logit_sig3']]\n",
    "data_df_y =  data_df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data_df_x,\n",
    "            data_df_y,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = data_df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-wrapping",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# copy/pasting in the best_estimator_ results\n",
    "# to avoid running another RandomizedSearch\n",
    "forest_tuned = RandomForestClassifier(bootstrap=False, max_depth=8, max_features='sqrt',\n",
    "                        min_samples_split=4, n_estimators=350, random_state=219)\n",
    "\n",
    "\n",
    "# FITTING the model object\n",
    "forest_tuned_fit = forest_tuned.fit(data_df_x, data_df_y)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "forest_tuned_pred = forest_tuned_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Forest Tuned Training ACCURACY:', forest_tuned.score(X_train, y_train).round(4))\n",
    "print('Forest Tuned Testing  ACCURACY:', forest_tuned.score(X_test, y_test).round(4))\n",
    "\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "forest_tuned_train_score = forest_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "forest_tuned_test_score  = forest_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Forest Train-Test Gap   :', abs(forest_tuned_train_score - \\\n",
    "                                       forest_tuned_test_score).round(4))\n",
    "\n",
    "forest_tuned_gap = abs(forest_tuned_train_score - forest_tuned_test_score).round(4)\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "rand_forest_tn, \\\n",
    "rand_forest_fp, \\\n",
    "rand_forest_fn, \\\n",
    "rand_forest_tp = confusion_matrix(y_true = y_test, y_pred = forest_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {rand_forest_tn}\n",
    "False Positives: {rand_forest_fp}\n",
    "False Negatives: {rand_forest_fn}\n",
    "True Positives : {rand_forest_tp}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "forest_tuned_auc = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = forest_tuned_pred).round(4) # auc\n",
    "\n",
    "\n",
    "print('Forest Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                                       y_score = forest_tuned_pred).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-surgery",
   "metadata": {},
   "source": [
    "# Gradient Boosted Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-restriction",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "data_df_x   =  data_df.loc[ : , candidate_dict['logit_sig3']]\n",
    "data_df_y =  data_df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data_df_x,\n",
    "            data_df_y,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = data_df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-tuning",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "gbm_tuned =  GradientBoostingClassifier(criterion='mse', loss='exponential', max_depth=2,\n",
    "                            max_features='sqrt', n_estimators=200,\n",
    "                            random_state=219)\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "gbm_tuned_fit = gbm_tuned.fit(data_df_x, data_df_y)\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "gbm_tuned_pred = gbm_tuned_fit.predict(X_test)\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', gbm_tuned_fit.score(X_train, y_train).round(4))\n",
    "print('Testing ACCURACY :', gbm_tuned_fit.score(X_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_gbm_train_score = gbm_tuned_fit.score(X_train, y_train).round(4) # accuracy\n",
    "full_gbm_test_score  = gbm_tuned_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Forest Train-Test Gap   :', abs(full_gbm_train_score - \\\n",
    "                                       full_gbm_test_score).round(4))\n",
    "\n",
    "full_gbm_gap = abs(full_gbm_train_score - full_gbm_test_score).round(4)\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "gbm_tuned_tn, \\\n",
    "gbm_tuned_fp, \\\n",
    "gbm_tuned_fn, \\\n",
    "gbm_tuned_tp = confusion_matrix(y_true = y_test, \n",
    "                                y_pred = gbm_tuned_pred).ravel()\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {gbm_tuned_tn}\n",
    "False Positives: {gbm_tuned_fp}\n",
    "False Negatives: {gbm_tuned_fn}\n",
    "True Positives : {gbm_tuned_tp}\n",
    "\"\"\")\n",
    "\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                  y_score = gbm_tuned_pred).round(4))\n",
    "\n",
    "# saving the AUC score\n",
    "gbm_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = gbm_tuned_pred).round(4) # auce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-funeral",
   "metadata": {},
   "source": [
    "# FINAL RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-victorian",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Name'        : ['Logistic Regression',\n",
    "                           'Classification Trees', \n",
    "                           'Random Forest [FINAL]',\n",
    "                           'Gradient Boosted Models'],\n",
    "           \n",
    "    'AUC Score'         : [logreg_auc_score, \n",
    "                           tree_tuned_auc, \n",
    "                           forest_tuned_auc,\n",
    "                           gbm_tuned_auc],\n",
    "    \n",
    "    'Training Accuracy' : [logreg_train_score, \n",
    "                           tree_tuned_train_score,\n",
    "                           forest_tuned_train_score,\n",
    "                           full_gbm_train_score],\n",
    "           \n",
    "    'Testing Accuracy'  : [logreg_test_score, \n",
    "                           tree_tuned_test_score,\n",
    "                           forest_tuned_test_score,\n",
    "                           full_gbm_test_score],\n",
    "    \n",
    "    'Train-Test Gap'    : [logreg_test_gap,\n",
    "                           tree_tuned_test_gap,\n",
    "                           forest_tuned_gap,\n",
    "                           full_gbm_gap],\n",
    "\n",
    "    'Confusion Matrix (TN, FP, FN, TP)'  : [(logreg_tn, logreg_fp, logreg_fn, logreg_tp),\n",
    "                         (full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp),\n",
    "                           (rand_forest_tn, rand_forest_fp, rand_forest_fn, rand_forest_tp),                 \n",
    "                           (gbm_tuned_tn, gbm_tuned_fp, gbm_tuned_fn, gbm_tuned_tp)]}\n",
    "\n",
    "                       \n",
    "# converting model_performance into a DataFrame\n",
    "model_performance = pd.DataFrame(model_performance)\n",
    "\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-depression",
   "metadata": {},
   "source": [
    "**For this project. I will be selecting Random Forest as my best model with AUC of 0.8416**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

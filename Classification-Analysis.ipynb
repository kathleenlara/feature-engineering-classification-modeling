{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "clear-ivory",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#importing my libraries\n",
    "import matplotlib.pyplot as plt                      \n",
    "import pandas as pd                                  \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import roc_auc_score       \n",
    "from sklearn.metrics import confusion_matrix  # predictive modeling with nice outputs \n",
    "import statsmodels.formula.api as smf # regression modeling\n",
    "import numpy as np\n",
    "\n",
    "# CART model packages\n",
    "from sklearn.tree import DecisionTreeClassifier   \n",
    "from sklearn.tree import export_graphviz   \n",
    "from io import StringIO\n",
    "from IPython.display import Image                  \n",
    "import pydotplus \n",
    "\n",
    "# new packages\n",
    "from sklearn.model_selection import RandomizedSearchCV    \n",
    "from sklearn.metrics import make_scorer  \n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier     # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gbm\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-morrison",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rural-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying file name\n",
    "file = 'Datasets/Apprentice_Chef_Dataset.xlsx'\n",
    "\n",
    "# reading the file into Python\n",
    "data_df = pd.read_excel(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-twist",
   "metadata": {},
   "source": [
    "## Missing Value Analysis and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "italian-pointer",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REVENUE                         0\n",
       "CROSS_SELL_SUCCESS              0\n",
       "NAME                            0\n",
       "EMAIL                           0\n",
       "FIRST_NAME                      0\n",
       "FAMILY_NAME                    47\n",
       "TOTAL_MEALS_ORDERED             0\n",
       "UNIQUE_MEALS_PURCH              0\n",
       "CONTACTS_W_CUSTOMER_SERVICE     0\n",
       "PRODUCT_CATEGORIES_VIEWED       0\n",
       "AVG_TIME_PER_SITE_VISIT         0\n",
       "MOBILE_NUMBER                   0\n",
       "CANCELLATIONS_BEFORE_NOON       0\n",
       "CANCELLATIONS_AFTER_NOON        0\n",
       "TASTES_AND_PREFERENCES          0\n",
       "PC_LOGINS                       0\n",
       "MOBILE_LOGINS                   0\n",
       "WEEKLY_PLAN                     0\n",
       "EARLY_DELIVERIES                0\n",
       "LATE_DELIVERIES                 0\n",
       "PACKAGE_LOCKER                  0\n",
       "REFRIGERATED_LOCKER             0\n",
       "AVG_PREP_VID_TIME               0\n",
       "LARGEST_ORDER_SIZE              0\n",
       "MASTER_CLASSES_ATTENDED         0\n",
       "MEDIAN_MEAL_RATING              0\n",
       "AVG_CLICKS_PER_VISIT            0\n",
       "TOTAL_PHOTOS_VIEWED             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "protecting-classics",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REVENUE                        0\n",
       "CROSS_SELL_SUCCESS             0\n",
       "NAME                           0\n",
       "EMAIL                          0\n",
       "FIRST_NAME                     0\n",
       "FAMILY_NAME                    0\n",
       "TOTAL_MEALS_ORDERED            0\n",
       "UNIQUE_MEALS_PURCH             0\n",
       "CONTACTS_W_CUSTOMER_SERVICE    0\n",
       "PRODUCT_CATEGORIES_VIEWED      0\n",
       "AVG_TIME_PER_SITE_VISIT        0\n",
       "MOBILE_NUMBER                  0\n",
       "CANCELLATIONS_BEFORE_NOON      0\n",
       "CANCELLATIONS_AFTER_NOON       0\n",
       "TASTES_AND_PREFERENCES         0\n",
       "PC_LOGINS                      0\n",
       "MOBILE_LOGINS                  0\n",
       "WEEKLY_PLAN                    0\n",
       "EARLY_DELIVERIES               0\n",
       "LATE_DELIVERIES                0\n",
       "PACKAGE_LOCKER                 0\n",
       "REFRIGERATED_LOCKER            0\n",
       "AVG_PREP_VID_TIME              0\n",
       "LARGEST_ORDER_SIZE             0\n",
       "MASTER_CLASSES_ATTENDED        0\n",
       "MEDIAN_MEAL_RATING             0\n",
       "AVG_CLICKS_PER_VISIT           0\n",
       "TOTAL_PHOTOS_VIEWED            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a dropped dataset to visualize 'FAMILY_NAME'\n",
    "df_dropped = data_df.dropna()\n",
    "\n",
    "#check if there is any missing values in the dropped dataset\n",
    "df_dropped.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recovered-kitty",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# creating an imputation value\n",
    "fill = 'Unknown'\n",
    "\n",
    "# imputing 'FAMILY_NAME'\n",
    "data_df['FAMILY_NAME'] = data_df['FAMILY_NAME'].fillna(fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-ultimate",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "metallic-spray",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOBILE_LOGINS_RATE</th>\n",
       "      <th>CANCELLATION_RATE</th>\n",
       "      <th>CATVIEWS_CLICKS_RATIO</th>\n",
       "      <th>CATVIEWS_VISIT_RATIO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.198265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.252908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.247647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MOBILE_LOGINS_RATE  CANCELLATION_RATE  CATVIEWS_CLICKS_RATIO  CATVIEWS_VISIT_RATIO\n",
       "0            0.142857           0.285714               0.588235              0.208333\n",
       "1            0.011494           0.000000               0.615385              0.198265\n",
       "2            0.066667           0.200000               0.312500              0.252908\n",
       "3            0.076923           0.153846               0.357143              0.055556\n",
       "4            0.021277           0.000000               0.833333              0.247647"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating New Calculated Variables\n",
    "data_df['MOBILE_LOGINS_RATE'] = data_df['MOBILE_LOGINS'] / data_df['TOTAL_MEALS_ORDERED']\n",
    "data_df['CANCELLATION_RATE'] = (data_df['CANCELLATIONS_BEFORE_NOON'] + data_df['CANCELLATIONS_AFTER_NOON'])/ data_df['TOTAL_MEALS_ORDERED']\n",
    "data_df['CATVIEWS_CLICKS_RATIO'] = data_df['PRODUCT_CATEGORIES_VIEWED'] / data_df['AVG_CLICKS_PER_VISIT']\n",
    "data_df['CATVIEWS_VISIT_RATIO'] = data_df['PRODUCT_CATEGORIES_VIEWED'] / data_df['AVG_TIME_PER_SITE_VISIT']\n",
    "\n",
    "\n",
    "# checking result\n",
    "data_df.loc[ : , ['MOBILE_LOGINS_RATE','CANCELLATION_RATE',\n",
    "                 'CATVIEWS_CLICKS_RATIO',\n",
    "                 'CATVIEWS_VISIT_RATIO']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-operation",
   "metadata": {},
   "source": [
    "## Continuous Variables: Trend Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "secret-huntington",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Log transformation on the continuos variables\n",
    "data_df['log_REVENUE'] = np.log10(data_df['REVENUE'])\n",
    "data_df['log_AVG_TIME_PER_SITE_VISIT'] = np.log10(data_df['AVG_TIME_PER_SITE_VISIT'])\n",
    "data_df['log_AVG_PREP_VID_TIME'] = np.log10(data_df['AVG_PREP_VID_TIME'])\n",
    "data_df['log_TOTAL_MEALS_ORDERED'] = np.log10(data_df['TOTAL_MEALS_ORDERED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fifteen-state",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_TOTAL_PHOTOS_VIEWED</th>\n",
       "      <th>has_WEEKLY_PLAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   has_TOTAL_PHOTOS_VIEWED  has_WEEKLY_PLAN\n",
       "0                        0                0\n",
       "1                        1                1\n",
       "2                        0                1\n",
       "3                        0                1\n",
       "4                        1                1\n",
       "5                        0                0\n",
       "6                        1                1\n",
       "7                        0                1\n",
       "8                        0                1\n",
       "9                        1                1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy variable for the 2 features\n",
    "data_df['has_TOTAL_PHOTOS_VIEWED'] = 0\n",
    "data_df['has_WEEKLY_PLAN'] = 0\n",
    "\n",
    "\n",
    "for index, value in data_df.iterrows():\n",
    "    \n",
    "    # TOTAL_PHOTOS_VIEWED\n",
    "    if data_df.loc[index, 'TOTAL_PHOTOS_VIEWED'] > 0:\n",
    "        data_df.loc[index, 'has_TOTAL_PHOTOS_VIEWED'] = 1\n",
    "\n",
    "\n",
    "    # Second_Flr_SF\n",
    "    if data_df.loc[index, 'WEEKLY_PLAN'] > 0:\n",
    "        data_df.loc[index, 'has_WEEKLY_PLAN'] = 1\n",
    "        \n",
    "        \n",
    "# checking results\n",
    "data_df['has_TOTAL_PHOTOS_VIEWED'].value_counts(normalize = False).sort_index()\n",
    "data_df['has_WEEKLY_PLAN'].value_counts(normalize = False).sort_index()\n",
    "\n",
    "data_df[['has_TOTAL_PHOTOS_VIEWED', 'has_WEEKLY_PLAN']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-speaking",
   "metadata": {},
   "source": [
    "## Interval / Count Variables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cleared-wallace",
   "metadata": {},
   "source": [
    "# Do a log transformation on interval and count variables that are skewed or has 0\n",
    "data_df['log_UNIQUE_MEALS_PURCH'] = np.log10(data_df['UNIQUE_MEALS_PURCH'])+ 0.01\n",
    "data_df['log_CONTACTS_W_CUSTOMER_SERVICE'] = np.log10(data_df['CONTACTS_W_CUSTOMER_SERVICE']) + 0.01\n",
    "data_df['log_PRODUCT_CATEGORIES_VIEWED'] = np.log10(data_df['PRODUCT_CATEGORIES_VIEWED'])+ 0.01\n",
    "data_df['log_LARGEST_ORDER_SIZE'] = np.log10(data_df['LARGEST_ORDER_SIZE']) + 0.01\n",
    "data_df['log_AVG_CLICKS_PER_VISIT'] = np.log10(data_df['AVG_CLICKS_PER_VISIT']) + 0.01\n",
    "data_df['log_MEDIAN_MEAL_RATING'] = np.log10(data_df['MEDIAN_MEAL_RATING'])+ 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "alpine-neutral",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Categorizing the Order size into 3 (Small, Medium, Large)\n",
    "\n",
    "data_df['ORDER_SIZE_SMALL'] = 0\n",
    "data_df['ORDER_SIZE_MEDIUM'] = 0\n",
    "data_df['ORDER_SIZE_LARGE'] = 0\n",
    "\n",
    "for index, value in data_df.iterrows():\n",
    "    if data_df.loc[index, 'LARGEST_ORDER_SIZE'] >= 8.00:\n",
    "        data_df.loc[index, 'ORDER_SIZE_LARGE'] = 1\n",
    "    elif data_df.loc[index, 'LARGEST_ORDER_SIZE'] >= 7.00:\n",
    "        data_df.loc[index, 'ORDER_SIZE_MEDIUM'] = 1\n",
    "    elif data_df.loc[index, 'LARGEST_ORDER_SIZE'] >= 4.00:\n",
    "        data_df.loc[index, 'ORDER_SIZE_SMALL'] = 1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "binary-savannah",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Creating new columns for dummy variable\n",
    "data_df['has_CANCELLATIONS_BEFORE_NOON']  = 0\n",
    "data_df['has_CANCELLATIONS_AFTER_NOON']   = 0\n",
    "data_df['has_MASTER_CLASSES_ATTENDED']    = 0\n",
    "data_df['has_EARLY_DELIVERIES']           = 0\n",
    "data_df['has_LATE_DELIVERIES']             = 0\n",
    "data_df['no_LATE_DELIVERIES']              = 0\n",
    "\n",
    "        \n",
    "# for loop to declare 0 and 1 \n",
    "for index, value in data_df.iterrows():\n",
    "    \n",
    "    # cancellations_before_noon\n",
    "    if data_df.loc[index,'CANCELLATIONS_BEFORE_NOON'] > 0:\n",
    "        data_df.loc[index, 'has_CANCELLATIONS_BEFORE_NOON'] = 1\n",
    "\n",
    "    # cancellations_after_noon\n",
    "    if data_df.loc[index, 'CANCELLATIONS_AFTER_NOON'] > 0:\n",
    "        data_df.loc[index, 'has_CANCELLATIONS_AFTER_NOON'] = 1\n",
    "        \n",
    "    # master_classes_attended\n",
    "    if data_df.loc[index, 'MASTER_CLASSES_ATTENDED'] > 0:\n",
    "        data_df.loc[index, 'has_MASTER_CLASSES_ATTENDED'] = 1\n",
    "        \n",
    "    # early_deliveries\n",
    "    if data_df.loc[index, 'EARLY_DELIVERIES'] > 0:\n",
    "        data_df.loc[index, 'has_EARLY_DELIVERIES'] = 1\n",
    "    \n",
    "    # late_deliveries\n",
    "    if data_df.loc[index, 'LATE_DELIVERIES'] > 0:\n",
    "        data_df.loc[index, 'has_LATE_DELIVERIES'] = 1\n",
    "\n",
    "    # no_late_deliveries\n",
    "    if data_df.loc[index,'LATE_DELIVERIES'] == 0:\n",
    "        data_df.loc[index, 'no_LATE_DELIVERIES'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-wagon",
   "metadata": {},
   "source": [
    "## Categorial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "secret-bride",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Splitting emails\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "# looping over each email address\n",
    "for index, col in data_df.iterrows():\n",
    "    \n",
    "    # splitting email domain at '@'\n",
    "    split_email = data_df.loc[index, 'EMAIL'].split(sep = '@')\n",
    "    \n",
    "    # appending placeholder_lst with the results\n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "\n",
    "# converting placeholder_lst into a DataFrame \n",
    "emails = pd.DataFrame(placeholder_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "illegal-genius",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gmail.com           303\n",
       "protonmail.com      284\n",
       "yahoo.com           274\n",
       "msn.com              72\n",
       "aol.com              69\n",
       "passport.com         64\n",
       "hotmail.com          63\n",
       "live.com             62\n",
       "me.com               59\n",
       "amex.com             30\n",
       "jnj.com              28\n",
       "merck.com            28\n",
       "cocacola.com         28\n",
       "mcdonalds.com        28\n",
       "apple.com            27\n",
       "nike.com             27\n",
       "ge.org               26\n",
       "ibm.com              26\n",
       "dupont.com           26\n",
       "microsoft.com        25\n",
       "chevron.com          25\n",
       "unitedhealth.com     24\n",
       "exxon.com            24\n",
       "travelers.com        24\n",
       "boeing.com           23\n",
       "pg.com               22\n",
       "verizon.com          22\n",
       "caterpillar.com      22\n",
       "mmm.com              22\n",
       "disney.com           21\n",
       "walmart.com          21\n",
       "pfizer.com           20\n",
       "visa.com             20\n",
       "jpmorgan.com         19\n",
       "unitedtech.com       18\n",
       "cisco.com            18\n",
       "goldmansacs.com      18\n",
       "homedepot.com        17\n",
       "intel.com            17\n",
       "Name: EMAIL_domain, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# renaming column to concatenate\n",
    "emails.columns = ['0' , 'EMAIL_domain']\n",
    "\n",
    "\n",
    "# concatenating personal_email_domain with friends DataFrame\n",
    "data_df = pd.concat([data_df, emails['EMAIL_domain']],\n",
    "                     axis = 1)\n",
    "\n",
    "\n",
    "# printing value counts of personal_email_domain\n",
    "data_df.loc[: ,'EMAIL_domain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "nuclear-youth",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "personal        861\n",
       "professional    696\n",
       "junk            389\n",
       "Name: domain_group, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating domain types \n",
    "\n",
    "# email domain types\n",
    "professional_emails = ['@amex.com','@jnj.com', '@merck.com', '@cocacola.com','@mcdonalds.com', '@apple.com',\n",
    "                              '@nike.com','@ge.org','@dupont.com','@ibm.com','@chevron.com','@microsoft.com','@exxon.com','@unitedhealth.com',\n",
    "                              '@travelers.com','@boeing.com','@mmm.com','@caterpillar.com','@verizon.com','@pg.com',\n",
    "                              '@walmart.com','@disney.com','@pfizer.com','@visa.com','@jpmorgan.com','@cisco.com',\n",
    "                              '@unitedtech.com','@goldmansacs.com','@homedepot.com','@intel.com']\n",
    "\n",
    "personal_emails  = ['@gmail.com', '@yahoo.com', '@protonmail.com']\n",
    "\n",
    "\n",
    "junk_email_domains       = ['@me.com',\n",
    "                            '@aol.com',\n",
    "                            '@hotmail.com',\n",
    "                            '@live.com',\n",
    "                            '@msn.com',\n",
    "                            '@passport.com']\n",
    "\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "\n",
    "# looping to group observations by domain type\n",
    "for domain in data_df['EMAIL_domain']:\n",
    "    \n",
    "    if '@' + domain in professional_emails:\n",
    "        placeholder_lst.append('professional')\n",
    "        \n",
    "\n",
    "    elif '@' + domain in personal_emails:\n",
    "        placeholder_lst.append('personal')\n",
    "        \n",
    "    elif '@' + domain in junk_email_domains:\n",
    "        placeholder_lst.append('junk')\n",
    "    \n",
    "    else:\n",
    "            print('Unknown')\n",
    "\n",
    "\n",
    "# concatenating with original DataFrame\n",
    "data_df['domain_group'] = pd.Series(placeholder_lst)\n",
    "\n",
    "\n",
    "# checking results\n",
    "data_df['domain_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "false-photographer",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# one hot encoding for email domain categorial variable\n",
    "one_hot_domain       = pd.get_dummies(data_df['domain_group'])\n",
    "\n",
    "\n",
    "# dropping categorical variables after they've been encoded\n",
    "data_df = data_df.drop('EMAIL', axis = 1)\n",
    "data_df = data_df.drop('domain_group', axis = 1)\n",
    "data_df = data_df.drop('EMAIL_domain', axis = 1)\n",
    "\n",
    "# joining codings together\n",
    "data_df = data_df.join([one_hot_domain])\n",
    "\n",
    "\n",
    "# saving new columns\n",
    "new_columns = data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sharp-runner",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Counting the number of names\n",
    "\n",
    "def mv_flagger(df):\n",
    "    \"\"\"\n",
    "Flags all columns that have missing values with 'm-COLUMN_NAME'.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "df : DataFrame to flag missing values\n",
    "\n",
    "\n",
    "RETURNS\n",
    "-------\n",
    "DataFrame with missing value flags.\"\"\"\n",
    "\n",
    "\n",
    "    for col in df:\n",
    "\n",
    "        if df[col].isnull().astype(int).sum() > 0:\n",
    "            df['m_'+col] = df[col].isnull().astype(int)\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "# text_split_feature\n",
    "#########################\n",
    "def text_split_feature(col, df, sep=' ', new_col_name='NUM_OF_NAMES'):\n",
    "    \"\"\"\n",
    "Splits values in a string Series (as part of a DataFrame) and sums the number\n",
    "of resulting items. Automatically appends summed column to original DataFrame.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "col          : column to split\n",
    "df           : DataFrame where column is located\n",
    "sep          : string sequence to split by, default ' '\n",
    "new_col_name : name of new column after summing split, default\n",
    "               'number_of_names'\n",
    "\"\"\"\n",
    "    \n",
    "    df[new_col_name] = 0\n",
    "    \n",
    "    \n",
    "    for index, val in df.iterrows():\n",
    "        df.loc[index, new_col_name] = len(df.loc[index, col].split(sep = ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "greater-nigeria",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     591\n",
       "2    1201\n",
       "3      98\n",
       "4       9\n",
       "5      35\n",
       "6      12\n",
       "Name: NUM_OF_NAMES, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling text_split_feature\n",
    "text_split_feature(col = 'NAME',\n",
    "                   df  = data_df)\n",
    "\n",
    "\n",
    "# checking results\n",
    "data_df['NUM_OF_NAMES'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-intelligence",
   "metadata": {},
   "source": [
    "# USER DEFINED FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "spread-beach",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# optimal_neighbors\n",
    "\n",
    "def optimal_neighbors(x_data,\n",
    "                      y_data,\n",
    "                      standardize = True,\n",
    "                      pct_test=0.25,\n",
    "                      seed=219,\n",
    "                      response_type='reg',\n",
    "                      max_neighbors=20,\n",
    "                      show_viz=True):\n",
    "    \"\"\"\n",
    "Exhaustively compute training and testing results for KNN across\n",
    "[1, max_neighbors]. Outputs the maximum test score and (by default) a\n",
    "visualization of the results.\n",
    "PARAMETERS\n",
    "----------\n",
    "x_data        : explanatory variable data\n",
    "y_data        : response variable\n",
    "standardize   : whether or not to standardize the X data, default True\n",
    "pct_test      : test size for training and validation from (0,1), default 0.25\n",
    "seed          : random seed to be used in algorithm, default 219\n",
    "response_type : type of neighbors algorithm to use, default 'reg'\n",
    "    Use 'reg' for regression (KNeighborsRegressor)\n",
    "    Use 'class' for classification (KNeighborsClassifier)\n",
    "max_neighbors : maximum number of neighbors in exhaustive search, default 20\n",
    "show_viz      : display or surpress k-neigbors visualization, default True\n",
    "\"\"\"    \n",
    "    \n",
    "    \n",
    "    if standardize == True:\n",
    "        # optionally standardizing x_data\n",
    "        scaler             = StandardScaler()\n",
    "        scaler.fit(x_data)\n",
    "        x_scaled           = scaler.transform(x_data)\n",
    "        x_scaled_df        = pd.DataFrame(x_scaled)\n",
    "        x_data             = x_scaled_df\n",
    "\n",
    "\n",
    "\n",
    "    # train-test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data,\n",
    "                                                        y_data,\n",
    "                                                        test_size = pct_test,\n",
    "                                                        random_state = seed)\n",
    "\n",
    "\n",
    "    # creating lists for training set accuracy and test set accuracy\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    \n",
    "    # setting neighbor range\n",
    "    neighbors_settings = range(1, max_neighbors + 1)\n",
    "\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # building the model based on response variable type\n",
    "        if response_type == 'reg':\n",
    "            clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "            clf.fit(x_train, y_train)\n",
    "            \n",
    "        elif response_type == 'class':\n",
    "            clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "            clf.fit(x_train, y_train)            \n",
    "            \n",
    "        else:\n",
    "            print(\"Error: response_type must be 'reg' or 'class'\")\n",
    "        \n",
    "        \n",
    "        # recording the training set accuracy\n",
    "        training_accuracy.append(clf.score(x_train, y_train))\n",
    "    \n",
    "        # recording the generalization accuracy\n",
    "        test_accuracy.append(clf.score(x_test, y_test))\n",
    "\n",
    "\n",
    "    # optionally displaying visualization\n",
    "    if show_viz == True:\n",
    "        # plotting the visualization\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "        plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "        plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(\"n_neighbors\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # returning optimal number of neighbors\n",
    "    print(f\"The optimal number of neighbors is: {test_accuracy.index(max(test_accuracy))+1}\")\n",
    "    return test_accuracy.index(max(test_accuracy))+1\n",
    "\n",
    "\n",
    "\n",
    "# visual_cm\n",
    "\n",
    "def visual_cm(true_y, pred_y, labels = None):\n",
    "    \"\"\"\n",
    "Creates a visualization of a confusion matrix.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "true_y : true values for the response variable\n",
    "pred_y : predicted values for the response variable\n",
    "labels : , default None\n",
    "    \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "interim-collection",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1321\n",
       "0     625\n",
       "Name: CROSS_SELL_SUCCESS, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CROSS_SELL_SUCCESS               1.000000\n",
       "professional                     0.194102\n",
       "CANCELLATIONS_BEFORE_NOON        0.163442\n",
       "NUM_OF_NAMES                     0.156373\n",
       "has_CANCELLATIONS_BEFORE_NOON    0.138611\n",
       "MOBILE_NUMBER                    0.102657\n",
       "CANCELLATION_RATE                0.095703\n",
       "TASTES_AND_PREFERENCES           0.081438\n",
       "REFRIGERATED_LOCKER              0.068321\n",
       "has_MASTER_CLASSES_ATTENDED      0.049939\n",
       "PC_LOGINS                        0.044462\n",
       "PACKAGE_LOCKER                   0.043534\n",
       "personal                         0.038841\n",
       "MASTER_CLASSES_ATTENDED          0.037213\n",
       "CONTACTS_W_CUSTOMER_SERVICE      0.036541\n",
       "log_AVG_PREP_VID_TIME            0.034529\n",
       "AVG_PREP_VID_TIME                0.032115\n",
       "MEDIAN_MEAL_RATING               0.031798\n",
       "ORDER_SIZE_SMALL                 0.026679\n",
       "log_TOTAL_MEALS_ORDERED          0.023570\n",
       "LARGEST_ORDER_SIZE               0.022247\n",
       "CATVIEWS_CLICKS_RATIO            0.019348\n",
       "log_AVG_TIME_PER_SITE_VISIT      0.015990\n",
       "EARLY_DELIVERIES                 0.015112\n",
       "AVG_TIME_PER_SITE_VISIT          0.011117\n",
       "has_TOTAL_PHOTOS_VIEWED          0.010868\n",
       "TOTAL_PHOTOS_VIEWED              0.010175\n",
       "log_REVENUE                      0.007533\n",
       "LATE_DELIVERIES                  0.006695\n",
       "TOTAL_MEALS_ORDERED              0.006475\n",
       "PRODUCT_CATEGORIES_VIEWED        0.004671\n",
       "REVENUE                          0.004540\n",
       "ORDER_SIZE_MEDIUM                0.003078\n",
       "has_LATE_DELIVERIES              0.001624\n",
       "UNIQUE_MEALS_PURCH               0.001136\n",
       "no_LATE_DELIVERIES              -0.001624\n",
       "WEEKLY_PLAN                     -0.005337\n",
       "has_EARLY_DELIVERIES            -0.006306\n",
       "ORDER_SIZE_LARGE                -0.009115\n",
       "CATVIEWS_VISIT_RATIO            -0.016933\n",
       "has_WEEKLY_PLAN                 -0.018820\n",
       "AVG_CLICKS_PER_VISIT            -0.035074\n",
       "MOBILE_LOGINS_RATE              -0.036504\n",
       "has_CANCELLATIONS_AFTER_NOON    -0.045200\n",
       "MOBILE_LOGINS                   -0.050647\n",
       "CANCELLATIONS_AFTER_NOON        -0.054149\n",
       "junk                            -0.280870\n",
       "Name: CROSS_SELL_SUCCESS, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking correlations with Y variable\n",
    "display(data_df.loc[:, \"CROSS_SELL_SUCCESS\"].value_counts())\n",
    "\n",
    "corr_scores = data_df.corr()\n",
    "\n",
    "corr_scores.loc[:, \"CROSS_SELL_SUCCESS\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-tournament",
   "metadata": {},
   "source": [
    "# TRAIN - TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "informal-bridal",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# declaring explanatory variables\n",
    "data_df_drop = ['CROSS_SELL_SUCCESS','NAME','FIRST_NAME', 'FAMILY_NAME']\n",
    "\n",
    "data_df_x = data_df.drop(data_df_drop, axis = 1)\n",
    "\n",
    "# declaring response variable\n",
    "data_df_y = data_df.loc[ : , 'CROSS_SELL_SUCCESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "unlikely-textbook",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# train-test split with stratification\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            data_df_x,\n",
    "            data_df_y,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = data_df_y)\n",
    "\n",
    "\n",
    "# merging training data for statsmodels\n",
    "data_df_train = pd.concat([x_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hidden-thesis",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " REVENUE + \n",
      " TOTAL_MEALS_ORDERED + \n",
      " UNIQUE_MEALS_PURCH + \n",
      " CONTACTS_W_CUSTOMER_SERVICE + \n",
      " PRODUCT_CATEGORIES_VIEWED + \n",
      " AVG_TIME_PER_SITE_VISIT + \n",
      " MOBILE_NUMBER + \n",
      " CANCELLATIONS_BEFORE_NOON + \n",
      " CANCELLATIONS_AFTER_NOON + \n",
      " TASTES_AND_PREFERENCES + \n",
      " PC_LOGINS + \n",
      " MOBILE_LOGINS + \n",
      " WEEKLY_PLAN + \n",
      " EARLY_DELIVERIES + \n",
      " LATE_DELIVERIES + \n",
      " PACKAGE_LOCKER + \n",
      " REFRIGERATED_LOCKER + \n",
      " AVG_PREP_VID_TIME + \n",
      " LARGEST_ORDER_SIZE + \n",
      " MASTER_CLASSES_ATTENDED + \n",
      " MEDIAN_MEAL_RATING + \n",
      " AVG_CLICKS_PER_VISIT + \n",
      " TOTAL_PHOTOS_VIEWED + \n",
      " MOBILE_LOGINS_RATE + \n",
      " CANCELLATION_RATE + \n",
      " CATVIEWS_CLICKS_RATIO + \n",
      " CATVIEWS_VISIT_RATIO + \n",
      " log_REVENUE + \n",
      " log_AVG_TIME_PER_SITE_VISIT + \n",
      " log_AVG_PREP_VID_TIME + \n",
      " log_TOTAL_MEALS_ORDERED + \n",
      " has_TOTAL_PHOTOS_VIEWED + \n",
      " has_WEEKLY_PLAN + \n",
      " ORDER_SIZE_SMALL + \n",
      " ORDER_SIZE_MEDIUM + \n",
      " ORDER_SIZE_LARGE + \n",
      " has_CANCELLATIONS_BEFORE_NOON + \n",
      " has_CANCELLATIONS_AFTER_NOON + \n",
      " has_MASTER_CLASSES_ATTENDED + \n",
      " has_EARLY_DELIVERIES + \n",
      " has_LATE_DELIVERIES + \n",
      " no_LATE_DELIVERIES + \n",
      " junk + \n",
      " personal + \n",
      " professional + \n",
      " NUM_OF_NAMES + \n"
     ]
    }
   ],
   "source": [
    "for val in data_df_x:\n",
    "    print(f\" {val} + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fuzzy-hostel",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.526777\n",
      "         Iterations 19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>CROSS_SELL_SUCCESS</td> <th>  No. Observations:  </th>  <td>  1459</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>Logit</td>       <th>  Df Residuals:      </th>  <td>  1414</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>MLE</td>        <th>  Df Model:          </th>  <td>    44</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 14 Feb 2021</td>  <th>  Pseudo R-squ.:     </th>  <td>0.1611</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:12:56</td>      <th>  Log-Likelihood:    </th> <td> -768.57</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>              <td>True</td>        <th>  LL-Null:           </th> <td> -916.19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>  LLR p-value:       </th> <td>6.267e-39</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                <td></td>                   <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                     <td>   -2.5357</td> <td> 4.44e+06</td> <td>-5.71e-07</td> <td> 1.000</td> <td>-8.71e+06</td> <td> 8.71e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REVENUE</th>                       <td>   -0.0002</td> <td>    0.000</td> <td>   -1.171</td> <td> 0.241</td> <td>   -0.001</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TOTAL_MEALS_ORDERED</th>           <td>   -0.0109</td> <td>    0.003</td> <td>   -3.171</td> <td> 0.002</td> <td>   -0.018</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UNIQUE_MEALS_PURCH</th>            <td>    0.0003</td> <td>    0.027</td> <td>    0.010</td> <td> 0.992</td> <td>   -0.053</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CONTACTS_W_CUSTOMER_SERVICE</th>   <td>    0.0930</td> <td>    0.039</td> <td>    2.403</td> <td> 0.016</td> <td>    0.017</td> <td>    0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PRODUCT_CATEGORIES_VIEWED</th>     <td>   -0.0319</td> <td>    0.112</td> <td>   -0.284</td> <td> 0.776</td> <td>   -0.252</td> <td>    0.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_TIME_PER_SITE_VISIT</th>       <td>    0.0011</td> <td>    0.002</td> <td>    0.534</td> <td> 0.593</td> <td>   -0.003</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MOBILE_NUMBER</th>                 <td>    0.9109</td> <td>    0.183</td> <td>    4.973</td> <td> 0.000</td> <td>    0.552</td> <td>    1.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CANCELLATIONS_BEFORE_NOON</th>     <td>    0.1780</td> <td>    0.081</td> <td>    2.199</td> <td> 0.028</td> <td>    0.019</td> <td>    0.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CANCELLATIONS_AFTER_NOON</th>      <td>   -0.4385</td> <td>    0.435</td> <td>   -1.008</td> <td> 0.314</td> <td>   -1.291</td> <td>    0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TASTES_AND_PREFERENCES</th>        <td>    0.3623</td> <td>    0.139</td> <td>    2.608</td> <td> 0.009</td> <td>    0.090</td> <td>    0.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PC_LOGINS</th>                     <td>    0.2442</td> <td>    0.110</td> <td>    2.213</td> <td> 0.027</td> <td>    0.028</td> <td>    0.460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MOBILE_LOGINS</th>                 <td>   -0.4042</td> <td>    0.184</td> <td>   -2.202</td> <td> 0.028</td> <td>   -0.764</td> <td>   -0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>WEEKLY_PLAN</th>                   <td>    0.0080</td> <td>    0.005</td> <td>    1.462</td> <td> 0.144</td> <td>   -0.003</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EARLY_DELIVERIES</th>              <td>    0.1420</td> <td>    0.047</td> <td>    3.034</td> <td> 0.002</td> <td>    0.050</td> <td>    0.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LATE_DELIVERIES</th>               <td>    0.0132</td> <td>    0.026</td> <td>    0.497</td> <td> 0.619</td> <td>   -0.039</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PACKAGE_LOCKER</th>                <td>    0.0069</td> <td>    0.151</td> <td>    0.046</td> <td> 0.963</td> <td>   -0.290</td> <td>    0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REFRIGERATED_LOCKER</th>           <td>    0.5030</td> <td>    0.243</td> <td>    2.073</td> <td> 0.038</td> <td>    0.027</td> <td>    0.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_PREP_VID_TIME</th>             <td>    0.0084</td> <td>    0.007</td> <td>    1.258</td> <td> 0.208</td> <td>   -0.005</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LARGEST_ORDER_SIZE</th>            <td>   -0.0367</td> <td>    0.115</td> <td>   -0.318</td> <td> 0.750</td> <td>   -0.262</td> <td>    0.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MASTER_CLASSES_ATTENDED</th>       <td>   -0.1097</td> <td>    0.238</td> <td>   -0.460</td> <td> 0.645</td> <td>   -0.577</td> <td>    0.357</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MEDIAN_MEAL_RATING</th>            <td>    0.0834</td> <td>    0.178</td> <td>    0.468</td> <td> 0.640</td> <td>   -0.266</td> <td>    0.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_CLICKS_PER_VISIT</th>          <td>    0.0008</td> <td>    0.068</td> <td>    0.011</td> <td> 0.991</td> <td>   -0.133</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TOTAL_PHOTOS_VIEWED</th>           <td>   -0.0004</td> <td>    0.001</td> <td>   -0.809</td> <td> 0.419</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MOBILE_LOGINS_RATE</th>            <td>    8.8782</td> <td>    5.828</td> <td>    1.523</td> <td> 0.128</td> <td>   -2.544</td> <td>   20.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CANCELLATION_RATE</th>             <td>    3.4138</td> <td>    2.447</td> <td>    1.395</td> <td> 0.163</td> <td>   -1.382</td> <td>    8.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CATVIEWS_CLICKS_RATIO</th>         <td>    0.4495</td> <td>    1.379</td> <td>    0.326</td> <td> 0.744</td> <td>   -2.253</td> <td>    3.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CATVIEWS_VISIT_RATIO</th>          <td>   -2.0709</td> <td>    2.102</td> <td>   -0.985</td> <td> 0.325</td> <td>   -6.191</td> <td>    2.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_REVENUE</th>                   <td>   -0.0325</td> <td>    1.211</td> <td>   -0.027</td> <td> 0.979</td> <td>   -2.407</td> <td>    2.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_AVG_TIME_PER_SITE_VISIT</th>   <td>   -0.6736</td> <td>    0.843</td> <td>   -0.799</td> <td> 0.424</td> <td>   -2.325</td> <td>    0.978</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_AVG_PREP_VID_TIME</th>         <td>   -1.4796</td> <td>    2.388</td> <td>   -0.620</td> <td> 0.536</td> <td>   -6.160</td> <td>    3.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_TOTAL_MEALS_ORDERED</th>       <td>    3.1441</td> <td>    0.936</td> <td>    3.359</td> <td> 0.001</td> <td>    1.310</td> <td>    4.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>has_TOTAL_PHOTOS_VIEWED</th>       <td>    0.1700</td> <td>    0.190</td> <td>    0.894</td> <td> 0.371</td> <td>   -0.203</td> <td>    0.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>has_WEEKLY_PLAN</th>               <td>   -0.1705</td> <td>    0.168</td> <td>   -1.016</td> <td> 0.310</td> <td>   -0.499</td> <td>    0.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ORDER_SIZE_SMALL</th>              <td>   -0.0304</td> <td>    0.244</td> <td>   -0.125</td> <td> 0.901</td> <td>   -0.508</td> <td>    0.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ORDER_SIZE_MEDIUM</th>             <td>   -0.0411</td> <td>    0.525</td> <td>   -0.078</td> <td> 0.938</td> <td>   -1.070</td> <td>    0.988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ORDER_SIZE_LARGE</th>              <td>   -0.3926</td> <td>    0.656</td> <td>   -0.599</td> <td> 0.549</td> <td>   -1.678</td> <td>    0.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>has_CANCELLATIONS_BEFORE_NOON</th> <td>    0.1329</td> <td>    0.180</td> <td>    0.737</td> <td> 0.461</td> <td>   -0.220</td> <td>    0.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>has_CANCELLATIONS_AFTER_NOON</th>  <td>    0.1362</td> <td>    0.526</td> <td>    0.259</td> <td> 0.796</td> <td>   -0.896</td> <td>    1.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>has_MASTER_CLASSES_ATTENDED</th>   <td>    0.3829</td> <td>    0.308</td> <td>    1.243</td> <td> 0.214</td> <td>   -0.221</td> <td>    0.987</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>has_EARLY_DELIVERIES</th>          <td>   -0.4371</td> <td>    0.213</td> <td>   -2.056</td> <td> 0.040</td> <td>   -0.854</td> <td>   -0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>has_LATE_DELIVERIES</th>           <td>   -1.2463</td> <td>    6e+06</td> <td>-2.08e-07</td> <td> 1.000</td> <td>-1.18e+07</td> <td> 1.18e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>no_LATE_DELIVERIES</th>            <td>   -1.2894</td> <td> 6.09e+06</td> <td>-2.12e-07</td> <td> 1.000</td> <td>-1.19e+07</td> <td> 1.19e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>junk</th>                          <td>   -1.9666</td> <td>  3.6e+06</td> <td>-5.46e-07</td> <td> 1.000</td> <td>-7.05e+06</td> <td> 7.05e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>personal</th>                      <td>   -0.5996</td> <td>  3.6e+06</td> <td>-1.67e-07</td> <td> 1.000</td> <td>-7.05e+06</td> <td> 7.05e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>professional</th>                  <td>    0.0305</td> <td>  3.6e+06</td> <td> 8.49e-09</td> <td> 1.000</td> <td>-7.05e+06</td> <td> 7.05e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NUM_OF_NAMES</th>                  <td>    0.5658</td> <td>    0.096</td> <td>    5.909</td> <td> 0.000</td> <td>    0.378</td> <td>    0.753</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:     CROSS_SELL_SUCCESS   No. Observations:                 1459\n",
       "Model:                          Logit   Df Residuals:                     1414\n",
       "Method:                           MLE   Df Model:                           44\n",
       "Date:                Sun, 14 Feb 2021   Pseudo R-squ.:                  0.1611\n",
       "Time:                        23:12:56   Log-Likelihood:                -768.57\n",
       "converged:                       True   LL-Null:                       -916.19\n",
       "Covariance Type:            nonrobust   LLR p-value:                 6.267e-39\n",
       "=================================================================================================\n",
       "                                    coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------\n",
       "Intercept                        -2.5357   4.44e+06  -5.71e-07      1.000   -8.71e+06    8.71e+06\n",
       "REVENUE                          -0.0002      0.000     -1.171      0.241      -0.001       0.000\n",
       "TOTAL_MEALS_ORDERED              -0.0109      0.003     -3.171      0.002      -0.018      -0.004\n",
       "UNIQUE_MEALS_PURCH                0.0003      0.027      0.010      0.992      -0.053       0.053\n",
       "CONTACTS_W_CUSTOMER_SERVICE       0.0930      0.039      2.403      0.016       0.017       0.169\n",
       "PRODUCT_CATEGORIES_VIEWED        -0.0319      0.112     -0.284      0.776      -0.252       0.188\n",
       "AVG_TIME_PER_SITE_VISIT           0.0011      0.002      0.534      0.593      -0.003       0.005\n",
       "MOBILE_NUMBER                     0.9109      0.183      4.973      0.000       0.552       1.270\n",
       "CANCELLATIONS_BEFORE_NOON         0.1780      0.081      2.199      0.028       0.019       0.337\n",
       "CANCELLATIONS_AFTER_NOON         -0.4385      0.435     -1.008      0.314      -1.291       0.414\n",
       "TASTES_AND_PREFERENCES            0.3623      0.139      2.608      0.009       0.090       0.635\n",
       "PC_LOGINS                         0.2442      0.110      2.213      0.027       0.028       0.460\n",
       "MOBILE_LOGINS                    -0.4042      0.184     -2.202      0.028      -0.764      -0.044\n",
       "WEEKLY_PLAN                       0.0080      0.005      1.462      0.144      -0.003       0.019\n",
       "EARLY_DELIVERIES                  0.1420      0.047      3.034      0.002       0.050       0.234\n",
       "LATE_DELIVERIES                   0.0132      0.026      0.497      0.619      -0.039       0.065\n",
       "PACKAGE_LOCKER                    0.0069      0.151      0.046      0.963      -0.290       0.303\n",
       "REFRIGERATED_LOCKER               0.5030      0.243      2.073      0.038       0.027       0.979\n",
       "AVG_PREP_VID_TIME                 0.0084      0.007      1.258      0.208      -0.005       0.022\n",
       "LARGEST_ORDER_SIZE               -0.0367      0.115     -0.318      0.750      -0.262       0.189\n",
       "MASTER_CLASSES_ATTENDED          -0.1097      0.238     -0.460      0.645      -0.577       0.357\n",
       "MEDIAN_MEAL_RATING                0.0834      0.178      0.468      0.640      -0.266       0.433\n",
       "AVG_CLICKS_PER_VISIT              0.0008      0.068      0.011      0.991      -0.133       0.135\n",
       "TOTAL_PHOTOS_VIEWED              -0.0004      0.001     -0.809      0.419      -0.001       0.001\n",
       "MOBILE_LOGINS_RATE                8.8782      5.828      1.523      0.128      -2.544      20.301\n",
       "CANCELLATION_RATE                 3.4138      2.447      1.395      0.163      -1.382       8.210\n",
       "CATVIEWS_CLICKS_RATIO             0.4495      1.379      0.326      0.744      -2.253       3.152\n",
       "CATVIEWS_VISIT_RATIO             -2.0709      2.102     -0.985      0.325      -6.191       2.050\n",
       "log_REVENUE                      -0.0325      1.211     -0.027      0.979      -2.407       2.342\n",
       "log_AVG_TIME_PER_SITE_VISIT      -0.6736      0.843     -0.799      0.424      -2.325       0.978\n",
       "log_AVG_PREP_VID_TIME            -1.4796      2.388     -0.620      0.536      -6.160       3.201\n",
       "log_TOTAL_MEALS_ORDERED           3.1441      0.936      3.359      0.001       1.310       4.979\n",
       "has_TOTAL_PHOTOS_VIEWED           0.1700      0.190      0.894      0.371      -0.203       0.543\n",
       "has_WEEKLY_PLAN                  -0.1705      0.168     -1.016      0.310      -0.499       0.158\n",
       "ORDER_SIZE_SMALL                 -0.0304      0.244     -0.125      0.901      -0.508       0.447\n",
       "ORDER_SIZE_MEDIUM                -0.0411      0.525     -0.078      0.938      -1.070       0.988\n",
       "ORDER_SIZE_LARGE                 -0.3926      0.656     -0.599      0.549      -1.678       0.892\n",
       "has_CANCELLATIONS_BEFORE_NOON     0.1329      0.180      0.737      0.461      -0.220       0.486\n",
       "has_CANCELLATIONS_AFTER_NOON      0.1362      0.526      0.259      0.796      -0.896       1.168\n",
       "has_MASTER_CLASSES_ATTENDED       0.3829      0.308      1.243      0.214      -0.221       0.987\n",
       "has_EARLY_DELIVERIES             -0.4371      0.213     -2.056      0.040      -0.854      -0.020\n",
       "has_LATE_DELIVERIES              -1.2463      6e+06  -2.08e-07      1.000   -1.18e+07    1.18e+07\n",
       "no_LATE_DELIVERIES               -1.2894   6.09e+06  -2.12e-07      1.000   -1.19e+07    1.19e+07\n",
       "junk                             -1.9666    3.6e+06  -5.46e-07      1.000   -7.05e+06    7.05e+06\n",
       "personal                         -0.5996    3.6e+06  -1.67e-07      1.000   -7.05e+06    7.05e+06\n",
       "professional                      0.0305    3.6e+06   8.49e-09      1.000   -7.05e+06    7.05e+06\n",
       "NUM_OF_NAMES                      0.5658      0.096      5.909      0.000       0.378       0.753\n",
       "=================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logit_full = smf.logit(formula = \"\"\" CROSS_SELL_SUCCESS ~  REVENUE + \n",
    " TOTAL_MEALS_ORDERED + \n",
    " UNIQUE_MEALS_PURCH + \n",
    " CONTACTS_W_CUSTOMER_SERVICE + \n",
    " PRODUCT_CATEGORIES_VIEWED + \n",
    " AVG_TIME_PER_SITE_VISIT + \n",
    " MOBILE_NUMBER + \n",
    " CANCELLATIONS_BEFORE_NOON + \n",
    " CANCELLATIONS_AFTER_NOON + \n",
    " TASTES_AND_PREFERENCES + \n",
    " PC_LOGINS + \n",
    " MOBILE_LOGINS + \n",
    " WEEKLY_PLAN + \n",
    " EARLY_DELIVERIES + \n",
    " LATE_DELIVERIES + \n",
    " PACKAGE_LOCKER + \n",
    " REFRIGERATED_LOCKER + \n",
    " AVG_PREP_VID_TIME + \n",
    " LARGEST_ORDER_SIZE + \n",
    " MASTER_CLASSES_ATTENDED + \n",
    " MEDIAN_MEAL_RATING + \n",
    " AVG_CLICKS_PER_VISIT + \n",
    " TOTAL_PHOTOS_VIEWED + \n",
    " MOBILE_LOGINS_RATE + \n",
    " CANCELLATION_RATE + \n",
    " CATVIEWS_CLICKS_RATIO + \n",
    " CATVIEWS_VISIT_RATIO + \n",
    " log_REVENUE + \n",
    " log_AVG_TIME_PER_SITE_VISIT + \n",
    " log_AVG_PREP_VID_TIME + \n",
    " log_TOTAL_MEALS_ORDERED + \n",
    " has_TOTAL_PHOTOS_VIEWED + \n",
    " has_WEEKLY_PLAN + \n",
    " ORDER_SIZE_SMALL + \n",
    " ORDER_SIZE_MEDIUM + \n",
    " ORDER_SIZE_LARGE + \n",
    " has_CANCELLATIONS_BEFORE_NOON + \n",
    " has_CANCELLATIONS_AFTER_NOON + \n",
    " has_MASTER_CLASSES_ATTENDED + \n",
    " has_EARLY_DELIVERIES + \n",
    " has_LATE_DELIVERIES + \n",
    " no_LATE_DELIVERIES + \n",
    " junk + \n",
    " personal + \n",
    " professional + \n",
    " NUM_OF_NAMES\n",
    "                                     \"\"\",\n",
    " data    = data_df_train)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "logit_full = logit_full.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "logit_full.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-tumor",
   "metadata": {},
   "source": [
    "# CANDIDATE DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bored-light",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# creating a dictionary to store candidate models\n",
    "# I tried different variables for different models and picked which set will perform better\n",
    "\n",
    "candidate_dict = {\n",
    "\n",
    " # full model\n",
    " 'logit_full'   : ['REVENUE',\n",
    "'TOTAL_MEALS_ORDERED',\n",
    "'UNIQUE_MEALS_PURCH',\n",
    "'CONTACTS_W_CUSTOMER_SERVICE',\n",
    "'PRODUCT_CATEGORIES_VIEWED',\n",
    "'AVG_TIME_PER_SITE_VISIT',\n",
    "'MOBILE_NUMBER',\n",
    "'CANCELLATIONS_BEFORE_NOON',\n",
    "'CANCELLATIONS_AFTER_NOON',\n",
    "'TASTES_AND_PREFERENCES',\n",
    "'PC_LOGINS',\n",
    "'MOBILE_LOGINS',\n",
    "'WEEKLY_PLAN',\n",
    "'EARLY_DELIVERIES',\n",
    "'LATE_DELIVERIES',\n",
    "'PACKAGE_LOCKER',\n",
    "'REFRIGERATED_LOCKER',\n",
    "'AVG_PREP_VID_TIME',\n",
    "'LARGEST_ORDER_SIZE',\n",
    "'MASTER_CLASSES_ATTENDED',\n",
    "'MEDIAN_MEAL_RATING',\n",
    "'AVG_CLICKS_PER_VISIT',\n",
    "'TOTAL_PHOTOS_VIEWED',\n",
    "'MOBILE_LOGINS_RATE',\n",
    "'CANCELLATION_RATE',\n",
    "'has_CANCELLATIONS_BEFORE_NOON',\n",
    "'has_CANCELLATIONS_AFTER_NOON',\n",
    "'has_WEEKLY_PLAN',\n",
    "'has_MASTER_CLASSES_ATTENDED',\n",
    "'has_EARLY_DELIVERIES',\n",
    "'has_LATE_DELIVERIES',\n",
    "'no_LATE_DELIVERIES',\n",
    "'professional',\n",
    "'personal',\n",
    "'junk',\n",
    "'MOBILE_TO_PC_RATIO',\n",
    "'CATVIEWS_CLICKS_RATIO',\n",
    "'CATVIEWS_VISIT_RATIO'],\n",
    " \n",
    "\n",
    "            \n",
    "# significant variables only (set 1)\n",
    " 'logit_sig'    : ['CONTACTS_W_CUSTOMER_SERVICE' ,\n",
    "                   'MOBILE_NUMBER' ,\n",
    "                   'TASTES_AND_PREFERENCES' ,\n",
    "                   'CANCELLATIONS_BEFORE_NOON' ,\n",
    "                   'PC_LOGINS' , \n",
    "                   'EARLY_DELIVERIES' ,\n",
    "                   'REFRIGERATED_LOCKER' ,\n",
    "                   'NUM_OF_NAMES',\n",
    "                   'junk'],\n",
    "\n",
    "\n",
    "# significant variables only (set 1)\n",
    " 'logit_sig2'    : ['TOTAL_MEALS_ORDERED', 'MOBILE_LOGINS',\n",
    "                    'WEEKLY_PLAN', 'has_MASTER_CLASSES_ATTENDED','PRODUCT_CATEGORIES_VIEWED', \n",
    "                    'CONTACTS_W_CUSTOMER_SERVICE',\n",
    "                   'MOBILE_NUMBER', 'TASTES_AND_PREFERENCES', 'CANCELLATIONS_BEFORE_NOON',\n",
    "                   'PC_LOGINS',  'EARLY_DELIVERIES','REFRIGERATED_LOCKER',\n",
    "                   'NUM_OF_NAMES','junk'],\n",
    "        \n",
    "    \n",
    "           \n",
    "# significant variables only (set 1)\n",
    " 'logit_sig3'    : ['TOTAL_MEALS_ORDERED', 'MOBILE_LOGINS','log_REVENUE',\n",
    "                    'WEEKLY_PLAN', 'has_MASTER_CLASSES_ATTENDED','PRODUCT_CATEGORIES_VIEWED', \n",
    "                    'CONTACTS_W_CUSTOMER_SERVICE', 'MOBILE_NUMBER',\n",
    "                   'TASTES_AND_PREFERENCES','CANCELLATIONS_BEFORE_NOON',\n",
    "                   'PC_LOGINS', 'EARLY_DELIVERIES','REFRIGERATED_LOCKER','has_EARLY_DELIVERIES',\n",
    "                   'NUM_OF_NAMES', 'junk','log_AVG_PREP_VID_TIME', 'ORDER_SIZE_SMALL','ORDER_SIZE_MEDIUM']\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-score",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "appointed-naples",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg Training ACCURACY: 0.7354\n",
      "LogReg Testing  ACCURACY: 0.729\n",
      "LogReg Train-Test Gap   : 0.0064\n"
     ]
    }
   ],
   "source": [
    "# train/test split with the full model\n",
    "data_df_x   =  data_df.loc[ : , candidate_dict['logit_sig']]\n",
    "data_df_y =  data_df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data_df_x,\n",
    "            data_df_y,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = data_df_y)\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            C = 1,\n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('LogReg Training ACCURACY:', logreg_fit.score(X_train, y_train).round(4))\n",
    "print('LogReg Testing  ACCURACY:', logreg_fit.score(X_test, y_test).round(4))\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(X_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('LogReg Train-Test Gap   :', abs(logreg_train_score - logreg_test_score).round(4))\n",
    "logreg_test_gap = abs(logreg_train_score - logreg_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "current-campaign",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 53\n",
      "False Positives: 103\n",
      "False Negatives: 29\n",
      "True Positives : 302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {logreg_tn}\n",
    "False Positives: {logreg_fp}\n",
    "False Negatives: {logreg_fn}\n",
    "True Positives : {logreg_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "experimental-given",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6261\n"
     ]
    }
   ],
   "source": [
    "# area under the roc curve (auc)\n",
    "print(roc_auc_score(y_true  = y_test,\n",
    "                    y_score = logreg_pred).round(decimals = 4))\n",
    "\n",
    "\n",
    "# saving AUC score for future use\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = logreg_pred).round(decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "effective-yemen",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intercept', -2.77)\n",
      "('CONTACTS_W_CUSTOMER_SERVICE', 0.05)\n",
      "('MOBILE_NUMBER', 0.81)\n",
      "('TASTES_AND_PREFERENCES', 0.37)\n",
      "('CANCELLATIONS_BEFORE_NOON', 0.28)\n",
      "('PC_LOGINS', 0.21)\n",
      "('EARLY_DELIVERIES', 0.06)\n",
      "('REFRIGERATED_LOCKER', 0.47)\n",
      "('NUM_OF_NAMES', 0.53)\n",
      "('junk', -1.54)\n"
     ]
    }
   ],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "logreg_model_values = zip(data_df[candidate_dict['logit_sig']].columns,\n",
    "                          logreg_fit.coef_.ravel().round(decimals = 2))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "logreg_model_lst = [('intercept', logreg_fit.intercept_[0].round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in logreg_model_values:\n",
    "    logreg_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in logreg_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-syndrome",
   "metadata": {},
   "source": [
    "# Classification: Full Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "north-pencil",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Tree Training ACCURACY: 1.0\n",
      "Full Tree Testing ACCURACY : 0.6468\n",
      "Full Tree AUC Score: 0.6029\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "full_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(x_train,\n",
    "                                                    y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(x_test,\n",
    "                                                    y_test).round(4))\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "removed-parade",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 75\n",
      "False Positives: 81\n",
      "False Negatives: 91\n",
      "True Positives : 240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "full_tree_tn, \\\n",
    "full_tree_fp, \\\n",
    "full_tree_fn, \\\n",
    "full_tree_tp = confusion_matrix(y_true = y_test, y_pred = full_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {full_tree_tn}\n",
    "False Positives: {full_tree_fp}\n",
    "False Negatives: {full_tree_fn}\n",
    "True Positives : {full_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-commerce",
   "metadata": {},
   "source": [
    "# Pruned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "recorded-blanket",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7505\n",
      "Testing  ACCURACY: 0.7598\n",
      "AUC Score        : 0.686\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "pruned_tree = DecisionTreeClassifier(max_depth = 4,\n",
    "                                     min_samples_leaf = 25,\n",
    "                                     random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "pruned_tree_fit  = pruned_tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "pruned_tree_pred = pruned_tree_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', pruned_tree_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', pruned_tree_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = pruned_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "pruned_tree_train_score = pruned_tree_fit.score(X_train, y_train).round(4) # accuracy\n",
    "pruned_tree_test_score  = pruned_tree_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving auc score\n",
    "pruned_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                        y_score = pruned_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "russian-invalid",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 75\n",
      "False Positives: 81\n",
      "False Negatives: 36\n",
      "True Positives : 295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "pruned_tree_tn, \\\n",
    "pruned_tree_fp, \\\n",
    "pruned_tree_fn, \\\n",
    "pruned_tree_tp = confusion_matrix(y_true = y_test, y_pred = pruned_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {pruned_tree_tn}\n",
    "False Positives: {pruned_tree_fp}\n",
    "False Negatives: {pruned_tree_fn}\n",
    "True Positives : {pruned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-armstrong",
   "metadata": {},
   "source": [
    "# RESULTS (WITHOUT TUNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "suffering-bidding",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model         AUC Score      TN, FP, FN, TP\n",
      "-----         ---------      --------------\n",
      "Logistic      0.6261         (53, 103, 29, 302)\n",
      "Full Tree     0.6029         (75, 81, 91, 240)\n",
      "Pruned Tree   0.686         (75, 81, 36, 295)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is NOT my final results\n",
    "\n",
    "# comparing results\n",
    "print(f\"\"\"\n",
    "Model         AUC Score      TN, FP, FN, TP\n",
    "-----         ---------      --------------\n",
    "Logistic      {logreg_auc_score}         {logreg_tn, logreg_fp, logreg_fn, logreg_tp}\n",
    "Full Tree     {full_tree_auc_score}         {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}\n",
    "Pruned Tree   {pruned_tree_auc_score}         {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Name'    : ['Logistic', 'Full Tree', 'Pruned Tree'],\n",
    "           \n",
    "    'AUC Score' : [logreg_auc_score, full_tree_auc_score, pruned_tree_auc_score],\n",
    "    \n",
    "    'Training Accuracy' : [logreg_train_score, full_tree_train_score,\n",
    "                           pruned_tree_train_score],\n",
    "           \n",
    "    'Testing Accuracy'  : [logreg_test_score, full_tree_test_score,\n",
    "                           pruned_tree_test_score],\n",
    "\n",
    "    'Confusion Matrix'  : [(logreg_tn, logreg_fp, logreg_fn, logreg_tp),\n",
    "                           (full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp),\n",
    "                           (pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-cedar",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acting-newspaper",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-82b7acaacc42>:3: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  C_space          = pd.np.arange(0.1, 5.0, 0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Parameters  : {'warm_start': True, 'solver': 'newton-cg', 'C': 3.0000000000000004}\n",
      "Tuned CV AUC      : 0.6365\n"
     ]
    }
   ],
   "source": [
    "# RandomizedSearchCV\n",
    "# declaring a hyperparameter space\n",
    "C_space          = pd.np.arange(0.1, 5.0, 0.1)\n",
    "warm_start_space = [True, False]\n",
    "solver_space     = ['newton-cg', 'sag', 'lbfgs']\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'C'          : C_space,\n",
    "              'warm_start' : warm_start_space,\n",
    "              'solver'     : solver_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "lr_tuned = LogisticRegression(random_state = 219,\n",
    "                              max_iter     = 1000)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "lr_tuned_cv = RandomizedSearchCV(estimator           = lr_tuned,   # the model object\n",
    "                                 param_distributions = param_grid, # parameters to tune\n",
    "                                 cv                  = 3,          # how many folds in cross-validation\n",
    "                                 n_iter              = 250,        # number of combinations of hyperparameters to try\n",
    "                                 random_state        = 219,        # starting point for random sequence\n",
    "                                 scoring = make_scorer(\n",
    "                                           roc_auc_score,\n",
    "                                           needs_threshold = False)) # scoring criteria (AUC)\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "lr_tuned_cv.fit(data_df_x, data_df_y)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", lr_tuned_cv.best_params_)\n",
    "print(\"Tuned CV AUC      :\", lr_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "single-pledge",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Tuned Training ACCURACY: 0.7375\n",
      "LR Tuned Testing  ACCURACY: 0.7351\n",
      "LR Tuned AUC Score        : 0.6357\n"
     ]
    }
   ],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "lr_tuned = lr_tuned_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step is not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "lr_tuned_pred = lr_tuned.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('LR Tuned Training ACCURACY:', lr_tuned.score(X_train, y_train).round(4))\n",
    "print('LR Tuned Testing  ACCURACY:', lr_tuned.score(X_test, y_test).round(4))\n",
    "print('LR Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = lr_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "lr_tuned_train_score = lr_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "lr_tuned_test_score  = lr_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "lr_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = lr_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-julian",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning on Classification Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "opponent-wagner",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-3b1b50e8b853>:4: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  depth_space     = pd.np.arange(1, 25, 1)\n",
      "<ipython-input-35-3b1b50e8b853>:5: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  leaf_space      = pd.np.arange(1, 100, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Parameters  : {'splitter': 'best', 'min_samples_leaf': 16, 'max_depth': 3, 'criterion': 'gini'}\n",
      "Tuned Training AUC: 0.7032\n"
     ]
    }
   ],
   "source": [
    "# declaring a hyperparameter space\n",
    "criterion_space = ['gini', 'entropy']\n",
    "splitter_space  = ['best', 'random']\n",
    "depth_space     = pd.np.arange(1, 25, 1)\n",
    "leaf_space      = pd.np.arange(1, 100, 1)\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'criterion'        : criterion_space,\n",
    "              'splitter'         : splitter_space,\n",
    "              'max_depth'        : depth_space,\n",
    "              'min_samples_leaf' : leaf_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "tuned_tree = DecisionTreeClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# RandomizedSearchCV object\n",
    "tuned_tree_cv = RandomizedSearchCV(estimator             = tuned_tree,\n",
    "                                   param_distributions   = param_grid,\n",
    "                                   cv                    = 3,\n",
    "                                   n_iter                = 1000,\n",
    "                                   random_state          = 219,\n",
    "                                   scoring = make_scorer(roc_auc_score,\n",
    "                                             needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "tuned_tree_cv.fit(data_df_x, data_df_y)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", tuned_tree_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", tuned_tree_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "strategic-soundtrack",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7402\n",
      "Testing  ACCURACY: 0.7762\n",
      "AUC Score        : 0.732\n",
      "Train-Test Gap   : 0.036\n"
     ]
    }
   ],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "tree_tuned = tuned_tree_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step is not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "tree_tuned_pred = tree_tuned.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', tree_tuned.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', tree_tuned.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = tree_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "tree_tuned_train_score = tree_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "tree_tuned_test_score  = tree_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Train-Test Gap   :', abs(tree_tuned_train_score - \\\n",
    "                                       tree_tuned_test_score).round(4))\n",
    "\n",
    "tree_tuned_test_gap = abs(tree_tuned_train_score - tree_tuned_test_score).round(4)\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "tree_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = tree_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "chicken-noise",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 95\n",
      "False Positives: 61\n",
      "False Negatives: 48\n",
      "True Positives : 283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "tuned_tree_tn, \\\n",
    "tuned_tree_fp, \\\n",
    "tuned_tree_fn, \\\n",
    "tuned_tree_tp = confusion_matrix(y_true = y_test, y_pred = tree_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {tuned_tree_tn}\n",
    "False Positives: {tuned_tree_fp}\n",
    "False Negatives: {tuned_tree_fn}\n",
    "True Positives : {tuned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-compiler",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "standard-colleague",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "data_df_x   =  data_df.loc[ : , candidate_dict['logit_sig3']]\n",
    "data_df_y =  data_df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data_df_x,\n",
    "            data_df_y,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = data_df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "celtic-wrapping",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest Tuned Training ACCURACY: 0.865\n",
      "Forest Tuned Testing  ACCURACY: 0.8953\n",
      "Forest Train-Test Gap   : 0.0303\n",
      "\n",
      "True Negatives : 108\n",
      "False Positives: 48\n",
      "False Negatives: 3\n",
      "True Positives : 328\n",
      "\n",
      "Forest Tuned AUC Score        : 0.8416\n"
     ]
    }
   ],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# copy/pasting in the best_estimator_ results\n",
    "# to avoid running another RandomizedSearch\n",
    "forest_tuned = RandomForestClassifier(bootstrap=False, max_depth=8, max_features='sqrt',\n",
    "                        min_samples_split=4, n_estimators=350, random_state=219)\n",
    "\n",
    "\n",
    "# FITTING the model object\n",
    "forest_tuned_fit = forest_tuned.fit(data_df_x, data_df_y)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "forest_tuned_pred = forest_tuned_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Forest Tuned Training ACCURACY:', forest_tuned.score(X_train, y_train).round(4))\n",
    "print('Forest Tuned Testing  ACCURACY:', forest_tuned.score(X_test, y_test).round(4))\n",
    "\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "forest_tuned_train_score = forest_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "forest_tuned_test_score  = forest_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Forest Train-Test Gap   :', abs(forest_tuned_train_score - \\\n",
    "                                       forest_tuned_test_score).round(4))\n",
    "\n",
    "forest_tuned_gap = abs(forest_tuned_train_score - forest_tuned_test_score).round(4)\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "rand_forest_tn, \\\n",
    "rand_forest_fp, \\\n",
    "rand_forest_fn, \\\n",
    "rand_forest_tp = confusion_matrix(y_true = y_test, y_pred = forest_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {rand_forest_tn}\n",
    "False Positives: {rand_forest_fp}\n",
    "False Negatives: {rand_forest_fn}\n",
    "True Positives : {rand_forest_tp}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "forest_tuned_auc = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = forest_tuned_pred).round(4) # auc\n",
    "\n",
    "\n",
    "print('Forest Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                                       y_score = forest_tuned_pred).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-surgery",
   "metadata": {},
   "source": [
    "# Gradient Boosted Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "unavailable-restriction",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "data_df_x   =  data_df.loc[ : , candidate_dict['logit_sig3']]\n",
    "data_df_y =  data_df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data_df_x,\n",
    "            data_df_y,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = data_df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "irish-tuning",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7855\n",
      "Testing ACCURACY : 0.7844\n",
      "Forest Train-Test Gap   : 0.0011\n",
      "\n",
      "True Negatives : 79\n",
      "False Positives: 77\n",
      "False Negatives: 28\n",
      "True Positives : 303\n",
      "\n",
      "AUC Score        : 0.7109\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "gbm_tuned =  GradientBoostingClassifier(criterion='mse', loss='exponential', max_depth=2,\n",
    "                            max_features='sqrt', n_estimators=200,\n",
    "                            random_state=219)\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "gbm_tuned_fit = gbm_tuned.fit(data_df_x, data_df_y)\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "gbm_tuned_pred = gbm_tuned_fit.predict(X_test)\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', gbm_tuned_fit.score(X_train, y_train).round(4))\n",
    "print('Testing ACCURACY :', gbm_tuned_fit.score(X_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_gbm_train_score = gbm_tuned_fit.score(X_train, y_train).round(4) # accuracy\n",
    "full_gbm_test_score  = gbm_tuned_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Forest Train-Test Gap   :', abs(full_gbm_train_score - \\\n",
    "                                       full_gbm_test_score).round(4))\n",
    "\n",
    "full_gbm_gap = abs(full_gbm_train_score - full_gbm_test_score).round(4)\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "gbm_tuned_tn, \\\n",
    "gbm_tuned_fp, \\\n",
    "gbm_tuned_fn, \\\n",
    "gbm_tuned_tp = confusion_matrix(y_true = y_test, \n",
    "                                y_pred = gbm_tuned_pred).ravel()\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {gbm_tuned_tn}\n",
    "False Positives: {gbm_tuned_fp}\n",
    "False Negatives: {gbm_tuned_fn}\n",
    "True Positives : {gbm_tuned_tp}\n",
    "\"\"\")\n",
    "\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                  y_score = gbm_tuned_pred).round(4))\n",
    "\n",
    "# saving the AUC score\n",
    "gbm_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = gbm_tuned_pred).round(4) # auce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-funeral",
   "metadata": {},
   "source": [
    "# FINAL RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "medieval-victorian",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>Train-Test Gap</th>\n",
       "      <th>Confusion Matrix (TN, FP, FN, TP)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6261</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.7290</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>(53, 103, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classification Trees</td>\n",
       "      <td>0.7320</td>\n",
       "      <td>0.7402</td>\n",
       "      <td>0.7762</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>(75, 81, 91, 240)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest [FINAL]</td>\n",
       "      <td>0.8416</td>\n",
       "      <td>0.8650</td>\n",
       "      <td>0.8953</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>(108, 48, 3, 328)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosted Models</td>\n",
       "      <td>0.7109</td>\n",
       "      <td>0.7855</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>(79, 77, 28, 303)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model Name  AUC Score  Training Accuracy  Testing Accuracy  Train-Test Gap Confusion Matrix (TN, FP, FN, TP)\n",
       "0      Logistic Regression     0.6261             0.7354            0.7290          0.0064                (53, 103, 29, 302)\n",
       "1     Classification Trees     0.7320             0.7402            0.7762          0.0360                 (75, 81, 91, 240)\n",
       "2    Random Forest [FINAL]     0.8416             0.8650            0.8953          0.0303                 (108, 48, 3, 328)\n",
       "3  Gradient Boosted Models     0.7109             0.7855            0.7844          0.0011                 (79, 77, 28, 303)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Name'        : ['Logistic Regression',\n",
    "                           'Classification Trees', \n",
    "                           'Random Forest [FINAL]',\n",
    "                           'Gradient Boosted Models'],\n",
    "           \n",
    "    'AUC Score'         : [logreg_auc_score, \n",
    "                           tree_tuned_auc, \n",
    "                           forest_tuned_auc,\n",
    "                           gbm_tuned_auc],\n",
    "    \n",
    "    'Training Accuracy' : [logreg_train_score, \n",
    "                           tree_tuned_train_score,\n",
    "                           forest_tuned_train_score,\n",
    "                           full_gbm_train_score],\n",
    "           \n",
    "    'Testing Accuracy'  : [logreg_test_score, \n",
    "                           tree_tuned_test_score,\n",
    "                           forest_tuned_test_score,\n",
    "                           full_gbm_test_score],\n",
    "    \n",
    "    'Train-Test Gap'    : [logreg_test_gap,\n",
    "                           tree_tuned_test_gap,\n",
    "                           forest_tuned_gap,\n",
    "                           full_gbm_gap],\n",
    "\n",
    "    'Confusion Matrix (TN, FP, FN, TP)'  : [(logreg_tn, logreg_fp, logreg_fn, logreg_tp),\n",
    "                         (full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp),\n",
    "                           (rand_forest_tn, rand_forest_fp, rand_forest_fn, rand_forest_tp),                 \n",
    "                           (gbm_tuned_tn, gbm_tuned_fp, gbm_tuned_fn, gbm_tuned_tp)]}\n",
    "\n",
    "                       \n",
    "# converting model_performance into a DataFrame\n",
    "model_performance = pd.DataFrame(model_performance)\n",
    "\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-depression",
   "metadata": {},
   "source": [
    "**For this project. I will be selecting Random Forest as my best model with AUC of 0.8416**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
